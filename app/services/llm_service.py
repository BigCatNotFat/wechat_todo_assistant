# -*- coding: utf-8 -*-
"""
å¤§æ¨¡å‹æœåŠ¡
å¤„ç†ä¸å¤§è¯­è¨€æ¨¡å‹çš„äº¤äº’ï¼ŒåŒ…æ‹¬Function Calling
"""
import json
from openai import OpenAI
from app.utils.llm_tools import TOOLS_SCHEMA, LLMTools

# Google Genai SDK (ç”¨äºGemini Google Search)
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    print("è­¦å‘Š: google-genai SDK æœªå®‰è£…ï¼ŒGemini Google Search åŠŸèƒ½å°†ä¸å¯ç”¨")


class LLMService:
    """å¤§æ¨¡å‹æœåŠ¡"""
    
    def __init__(self, config, prompt_manager, todo_service):
        """
        åˆå§‹åŒ–å¤§æ¨¡å‹æœåŠ¡
        
        Args:
            config: é…ç½®å¯¹è±¡
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
            todo_service: å¾…åŠäº‹é¡¹æœåŠ¡
        """
        self.config = config
        self.prompt_manager = prompt_manager
        self.todo_service = todo_service
        
        # è·å–å½“å‰æ¨¡å‹é…ç½®
        current_llm = config['CURRENT_LLM']
        llm_config = config['LLM_MODELS'][current_llm]
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ Google Genai SDK
        self.use_genai_sdk = llm_config.get('use_genai_sdk', False)
        self.use_google_search = llm_config.get('use_google_search', False)
        self.support_vision = llm_config.get('support_vision', False)
        
        # åˆå§‹åŒ–å®¢æˆ·ç«¯
        if self.use_genai_sdk and GENAI_AVAILABLE:
            # ä½¿ç”¨ Google Genai SDK
            self.genai_client = genai.Client(api_key=config['LLM_API_KEY'])
            self.client = None
            print(f"ä½¿ç”¨ Google Genai SDKï¼ŒGoogle Search: {'å¯ç”¨' if self.use_google_search else 'ç¦ç”¨'}")
        else:
            # ä½¿ç”¨ OpenAI SDKï¼ˆå…¼å®¹å¤šç§APIï¼‰
            self.client = OpenAI(
                api_key=config['LLM_API_KEY'],
                base_url=config['LLM_API_BASE']
            )
            self.genai_client = None
            self.use_google_search = False  # OpenAI SDK ä¸æ”¯æŒ Google Search
            print(f"ä½¿ç”¨ OpenAI å…¼å®¹ SDK")
        
        self.model = config['LLM_MODEL']
        self.temperature = config['LLM_TEMPERATURE']
        self.max_tokens = config['LLM_MAX_TOKENS']
    
    def _chat_with_genai_sdk(self, user_id, user_message, conversation_history=None):
        """
        ä½¿ç”¨ Google Genai SDK è¿›è¡Œå¯¹è¯ï¼ˆæ”¯æŒ Google Search å’Œå›¾ç‰‡ç†è§£ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰ï¼Œå¯èƒ½åŒ…å«å›¾ç‰‡
            
        Returns:
            å¤§æ¨¡å‹çš„å›å¤æ–‡æœ¬
        """
        try:
            # Tokenç»Ÿè®¡
            total_prompt_tokens = 0
            total_completion_tokens = 0
            total_tokens = 0
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = self.prompt_manager.get_prompt('system_prompt')
            
            # æ„å»ºcontentsåˆ—è¡¨ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
            contents = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            if system_prompt:
                contents.append(system_prompt + "\n\n")
            
            # å¤„ç†å¯¹è¯å†å²ï¼ˆåŒ…æ‹¬å›¾ç‰‡ï¼‰
            has_images = False
            if conversation_history:
                for msg in conversation_history:
                    role = msg.get('role', '')
                    content = msg.get('content', '')
                    image_data = msg.get('image_data')
                    
                    # æ·»åŠ æ–‡æœ¬
                    if role == 'user':
                        contents.append(f"ç”¨æˆ·: {content}\n")
                    elif role == 'assistant':
                        contents.append(f"åŠ©æ‰‹: {content}\n")
                    
                    # å¦‚æœæœ‰å›¾ç‰‡ä¸”æ”¯æŒvisionï¼Œæ·»åŠ å›¾ç‰‡
                    if image_data and self.support_vision:
                        has_images = True
                        try:
                            image_part = types.Part.from_bytes(
                                data=image_data['bytes'],
                                mime_type=image_data['mime_type']
                            )
                            contents.append(image_part)
                            contents.append("[ç”¨æˆ·å‘é€äº†å›¾ç‰‡]\n")
                        except Exception as e:
                            print(f"æ·»åŠ å†å²å›¾ç‰‡å¤±è´¥: {e}")
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            contents.append(f"ç”¨æˆ·: {user_message}")
            
            if has_images:
                print(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°å¯¹è¯å†å²ä¸­åŒ…å« {has_images} å¼ å›¾ç‰‡ï¼Œå°†å‘é€ç»™æ¨¡å‹")
            
            # é…ç½®å·¥å…·
            tools = []
            if self.use_google_search:
                # æ·»åŠ  Google Search å·¥å…·
                grounding_tool = types.Tool(google_search=types.GoogleSearch())
                tools.append(grounding_tool)
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            config = types.GenerateContentConfig(
                temperature=self.temperature,
                max_output_tokens=self.max_tokens,
                tools=tools if tools else None
            )
            
            # è°ƒç”¨ Genai API
            print(f"è°ƒç”¨ Gemini APIï¼Œæ¨¡å‹: {self.model}ï¼ŒGoogle Search: {self.use_google_search}ï¼ŒVision: {self.support_vision}")
            response = self.genai_client.models.generate_content(
                model=self.model,
                contents=contents,
                config=config
            )
            
            # ç»Ÿè®¡ tokenï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                usage = response.usage_metadata
                if hasattr(usage, 'prompt_token_count'):
                    total_prompt_tokens = usage.prompt_token_count
                if hasattr(usage, 'candidates_token_count'):
                    total_completion_tokens = usage.candidates_token_count
                if hasattr(usage, 'total_token_count'):
                    total_tokens = usage.total_token_count
                print(f"ç¬¬1è½®è°ƒç”¨ - è¾“å…¥token: {total_prompt_tokens}, è¾“å‡ºtoken: {total_completion_tokens}")
            
            # å¤„ç†å›ç­”
            answer_text = response.text
            
            # æ‰“å° Token ç»Ÿè®¡
            print(f"=" * 50)
            print(f"æœ¬æ¬¡å¯¹è¯Tokenç»Ÿè®¡:")
            print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
            print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
            print(f"  æ€»è®¡token: {total_tokens}")
            
            # å¦‚æœå¯ç”¨äº† Google Searchï¼Œæ‰“å°æœç´¢ä¿¡æ¯
            if self.use_google_search and hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                    metadata = candidate.grounding_metadata
                    print(f"\nğŸ“Š Google Search ä¿¡æ¯:")
                    
                    # æ‰“å°æœç´¢æŸ¥è¯¢
                    if hasattr(metadata, 'web_search_queries') and metadata.web_search_queries:
                        print(f"  æœç´¢æŸ¥è¯¢: {metadata.web_search_queries}")
                    
                    # æ‰“å°æ¥æºæ•°é‡
                    if hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                        print(f"  å‚è€ƒæ¥æºæ•°é‡: {len(metadata.grounding_chunks)}")
                        # æ‰“å°å‰3ä¸ªæ¥æº
                        for i, chunk in enumerate(metadata.grounding_chunks[:3]):
                            if hasattr(chunk, 'web') and chunk.web:
                                print(f"    [{i+1}] {chunk.web.title}: {chunk.web.uri}")
            
            print(f"=" * 50)
            
            return answer_text
            
        except Exception as e:
            print(f"Gemini API è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def chat_with_function_calling(self, user_id, user_message, conversation_history=None):
        """
        ä¸å¤§æ¨¡å‹å¯¹è¯ï¼Œæ”¯æŒFunction Calling
        
        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤§æ¨¡å‹çš„å›å¤æ–‡æœ¬
        """
        # å¦‚æœä½¿ç”¨ Google Genai SDKï¼Œè°ƒç”¨ä¸“é—¨çš„æ–¹æ³•
        if self.use_genai_sdk:
            return self._chat_with_genai_sdk(user_id, user_message, conversation_history)
        
        # ä»¥ä¸‹æ˜¯åŸæœ‰çš„ OpenAI SDK å®ç°
        try:
            # Tokenç»Ÿè®¡
            total_prompt_tokens = 0
            total_completion_tokens = 0
            total_tokens = 0
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            system_prompt = self.prompt_manager.get_prompt('system_prompt')
            if system_prompt:
                messages.append({
                    "role": "system",
                    "content": system_prompt
                })
            
            # æ·»åŠ å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
            if conversation_history:
                messages.extend(conversation_history)
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({
                "role": "user",
                "content": user_message
            })
            
            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=TOOLS_SCHEMA,
                tool_choice="auto",
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            # ç»Ÿè®¡ç¬¬ä¸€æ¬¡è°ƒç”¨çš„token
            if hasattr(response, 'usage') and response.usage:
                total_prompt_tokens += response.usage.prompt_tokens
                total_completion_tokens += response.usage.completion_tokens
                total_tokens += response.usage.total_tokens
                print(f"ç¬¬1è½®è°ƒç”¨ - è¾“å…¥token: {response.usage.prompt_tokens}, è¾“å‡ºtoken: {response.usage.completion_tokens}")
            
            # åˆ›å»ºå·¥å…·å®ä¾‹ï¼ˆç”¨äºæ‰§è¡Œå‡½æ•°è°ƒç”¨ï¼‰
            llm_tools = LLMTools(self.todo_service, user_id)
            
            # æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæœ€å¤š5è½®ï¼Œé˜²æ­¢æ— é™å¾ªç¯ï¼‰
            max_iterations = 5
            for iteration in range(max_iterations):
                # å¤„ç†å“åº”
                assistant_message = response.choices[0].message
                
                # å¦‚æœå¤§æ¨¡å‹éœ€è¦è°ƒç”¨å‡½æ•°
                if assistant_message.tool_calls:
                    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                    messages.append(assistant_message)
                    
                    # æ‰§è¡Œæ‰€æœ‰å‡½æ•°è°ƒç”¨
                    for tool_call in assistant_message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)
                        
                        print(f"æ‰§è¡Œå‡½æ•°è°ƒç”¨: {function_name}({function_args})")
                        
                        # æ‰§è¡Œå‡½æ•°
                        function_result = llm_tools.execute_tool_call(function_name, function_args)
                        
                        # æ·»åŠ å‡½æ•°ç»“æœåˆ°æ¶ˆæ¯åˆ—è¡¨
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps(function_result, ensure_ascii=False)
                        })
                    
                    # å†æ¬¡è°ƒç”¨å¤§æ¨¡å‹ï¼Œè®©å®ƒåŸºäºå‡½æ•°ç»“æœç”Ÿæˆå›å¤æˆ–ç»§ç»­è°ƒç”¨å·¥å…·
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        tools=TOOLS_SCHEMA,
                        tool_choice="auto",
                        temperature=self.temperature,
                        max_tokens=self.max_tokens
                    )
                    
                    # ç»Ÿè®¡åç»­è°ƒç”¨çš„token
                    if hasattr(response, 'usage') and response.usage:
                        total_prompt_tokens += response.usage.prompt_tokens
                        total_completion_tokens += response.usage.completion_tokens
                        total_tokens += response.usage.total_tokens
                        print(f"ç¬¬{iteration + 2}è½®è°ƒç”¨ - è¾“å…¥token: {response.usage.prompt_tokens}, è¾“å‡ºtoken: {response.usage.completion_tokens}")
                    
                    # ç»§ç»­å¾ªç¯ï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ–°çš„å·¥å…·è°ƒç”¨
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨äº†ï¼Œè¿”å›æœ€ç»ˆå›å¤
                    print(f"=" * 50)
                    print(f"æœ¬æ¬¡å¯¹è¯Tokenç»Ÿè®¡:")
                    print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
                    print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
                    print(f"  æ€»è®¡token: {total_tokens}")
                    print(f"=" * 50)
                    return assistant_message.content
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè¿”å›æœ€åçš„å›å¤
            print(f"è­¦å‘Šï¼šè¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå¼ºåˆ¶è¿”å›")
            print(f"=" * 50)
            print(f"æœ¬æ¬¡å¯¹è¯Tokenç»Ÿè®¡:")
            print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
            print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
            print(f"  æ€»è®¡token: {total_tokens}")
            print(f"=" * 50)
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def generate_daily_plan(self, user_id):
        """
        ç”Ÿæˆæ¯æ—¥ä»»åŠ¡è§„åˆ’
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            è§„åˆ’æ–‡æœ¬
        """
        try:
            # è·å–æ˜¨å¤©å®Œæˆçš„ä»»åŠ¡
            yesterday_todos = self.todo_service.get_yesterday_completed_todos(user_id)
            yesterday_summary = "\n".join([
                f"- {todo.content}" for todo in yesterday_todos
            ]) if yesterday_todos else "æ— "
            
            # è·å–ä»Šå¤©çš„å¾…åŠä»»åŠ¡
            today_todos = self.todo_service.get_today_todos(user_id)
            today_tasks = "\n".join([
                f"- [{todo.priority}] {todo.content}" for todo in today_todos
            ]) if today_todos else "æ— "
            
            # è·å–è§„åˆ’æç¤ºè¯
            prompt = self.prompt_manager.get_prompt(
                'daily_planning_prompt',
                yesterday_summary=yesterday_summary,
                today_tasks=today_tasks
            )
            
            # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆè§„åˆ’
            messages = [
                {"role": "system", "content": self.prompt_manager.get_prompt('system_prompt')},
                {"role": "user", "content": prompt}
            ]
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            # ç»Ÿè®¡tokenä½¿ç”¨
            if hasattr(response, 'usage') and response.usage:
                print(f"=" * 50)
                print(f"æ¯æ—¥è§„åˆ’Tokenç»Ÿè®¡:")
                print(f"  è¾“å…¥token: {response.usage.prompt_tokens}")
                print(f"  è¾“å‡ºtoken: {response.usage.completion_tokens}")
                print(f"  æ€»è®¡token: {response.usage.total_tokens}")
                print(f"=" * 50)
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"ç”Ÿæˆæ¯æ—¥è§„åˆ’å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆä»Šæ—¥è§„åˆ’ã€‚"

