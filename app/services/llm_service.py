# -*- coding: utf-8 -*-
"""
å¤§æ¨¡å‹æœåŠ¡
å¤„ç†ä¸å¤§è¯­è¨€æ¨¡å‹çš„äº¤äº’ï¼ŒåŒ…æ‹¬Function Calling
"""
import json
from datetime import datetime
import pytz
from openai import OpenAI
from app.utils.llm_tools import TOOLS_SCHEMA, LLMTools

# Google Genai SDK (ç”¨äºGemini Google Search)
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    print("è­¦å‘Š: google-genai SDK æœªå®‰è£…ï¼ŒGemini Google Search åŠŸèƒ½å°†ä¸å¯ç”¨")


class LLMService:
    """å¤§æ¨¡å‹æœåŠ¡"""
    
    def __init__(self, config, prompt_manager, todo_service, transaction_service=None):
        """
        åˆå§‹åŒ–å¤§æ¨¡å‹æœåŠ¡
        
        Args:
            config: é…ç½®å¯¹è±¡
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
            todo_service: å¾…åŠäº‹é¡¹æœåŠ¡
            transaction_service: è®°è´¦æœåŠ¡ï¼ˆå¯é€‰ï¼‰
        """
        self.config = config
        self.prompt_manager = prompt_manager
        self.todo_service = todo_service
        self.transaction_service = transaction_service
        
        # è·å–å½“å‰æ¨¡å‹é…ç½®
        current_llm = config['CURRENT_LLM']
        llm_config = config['LLM_MODELS'][current_llm]
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ Google Genai SDK
        self.use_genai_sdk = llm_config.get('use_genai_sdk', False)
        self.use_google_search = llm_config.get('use_google_search', False)
        
        # åˆå§‹åŒ–ä¸»æ¨¡å‹å®¢æˆ·ç«¯
        if self.use_genai_sdk and GENAI_AVAILABLE:
            # ä½¿ç”¨ Google Genai SDK
            self.genai_client = genai.Client(api_key=config['LLM_API_KEY'])
            self.client = None
            print(f"âœ… ä½¿ç”¨ Google Genai SDK - ä¸»æ¨¡å‹: {config['LLM_MODEL']}")
        else:
            # ä½¿ç”¨ OpenAI SDKï¼ˆå…¼å®¹å¤šç§APIï¼‰
            self.client = OpenAI(
                api_key=config['LLM_API_KEY'],
                base_url=config['LLM_API_BASE']
            )
            self.genai_client = None
            print(f"âœ… ä½¿ç”¨ OpenAI å…¼å®¹ SDK - ä¸»æ¨¡å‹: {config['LLM_MODEL']}")
        
        self.model = config['LLM_MODEL']
        self.temperature = config['LLM_TEMPERATURE']
        self.max_tokens = config['LLM_MAX_TOKENS']
        
        # æ€è€ƒé…ç½®ï¼ˆä»…é€‚ç”¨äº Gemini 2.5 ç³»åˆ—æ¨¡å‹ï¼‰
        self.thinking_budget = llm_config.get('thinking_budget', None)
        self.include_thoughts = llm_config.get('include_thoughts', False)
        
        if self.thinking_budget is not None and self.use_genai_sdk:
            budget_desc = "åŠ¨æ€æ€è€ƒ" if self.thinking_budget == -1 else f"{self.thinking_budget} tokens"
            thoughts_desc = "å¯ç”¨" if self.include_thoughts else "ç¦ç”¨"
            print(f"ğŸ§  æ€è€ƒæ¨¡å¼: é¢„ç®—={budget_desc}, æ€»ç»“è¾“å‡º={thoughts_desc}")
        
        # åˆå§‹åŒ–ç‹¬ç«‹çš„æœç´¢å®¢æˆ·ç«¯ï¼ˆå¦‚æœä¸»æ¨¡å‹å¯ç”¨äº†æœç´¢åŠŸèƒ½ï¼‰
        self.search_client = None
        self.search_model = None
        self.search_temperature = None
        
        if self.use_google_search and GENAI_AVAILABLE:
            # ä»é…ç½®ä¸­è·å–æœç´¢æ¨¡å‹å‚æ•°
            search_config = config.get('SEARCH_MODEL_CONFIG', {})
            search_api_key = search_config.get('api_key')
            
            if search_api_key:
                # åˆå§‹åŒ–æœç´¢ä¸“ç”¨å®¢æˆ·ç«¯ï¼ˆæ‰€æœ‰å‚æ•°ä»é…ç½®è¯»å–ï¼‰
                self.search_client = genai.Client(api_key=search_api_key)
                self.search_model = search_config.get('model', 'gemini-2.0-flash-exp')
                self.search_temperature = search_config.get('temperature', 0.7)  # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ï¼Œé»˜è®¤0.7
                print(f"ğŸ” å·²å¯ç”¨ç½‘ç»œæœç´¢åŠŸèƒ½ - æœç´¢æ¨¡å‹: {self.search_model}, æ¸©åº¦: {self.search_temperature}")
            else:
                print(f"âš ï¸ è­¦å‘Š: ä¸»æ¨¡å‹å·²å¼€å¯æœç´¢åŠŸèƒ½ï¼Œä½†æœªé…ç½® SEARCH_MODEL_CONFIG")
                self.use_google_search = False
        elif self.use_google_search and not GENAI_AVAILABLE:
            print(f"âš ï¸ è­¦å‘Š: ä¸»æ¨¡å‹å·²å¼€å¯æœç´¢åŠŸèƒ½ï¼Œä½† google-genai SDK æœªå®‰è£…")
            self.use_google_search = False
    
    def _convert_openai_tools_to_genai(self, openai_tools):
        """
        å°† OpenAI æ ¼å¼çš„å·¥å…·å®šä¹‰è½¬æ¢ä¸º Google Genai SDK æ ¼å¼
        
        Args:
            openai_tools: OpenAI æ ¼å¼çš„å·¥å…·åˆ—è¡¨
            
        Returns:
            Google Genai SDK æ ¼å¼çš„å‡½æ•°å£°æ˜åˆ—è¡¨
        """
        if not GENAI_AVAILABLE:
            return []
        
        function_declarations = []
        
        for tool in openai_tools:
            if tool.get('type') == 'function':
                func = tool['function']
                
                # è½¬æ¢ä¸º Genai æ ¼å¼
                function_declaration = {
                    'name': func['name'],
                    'description': func['description'],
                    'parameters': func['parameters']
                }
                
                function_declarations.append(function_declaration)
        
        return function_declarations
    
    def _chat_with_genai_sdk(self, user_id, user_message, conversation_history=None):
        """
        ä½¿ç”¨ Google Genai SDK è¿›è¡Œå¯¹è¯ï¼ˆæ”¯æŒ Google Search å’Œ Function Callingï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤§æ¨¡å‹çš„å›å¤æ–‡æœ¬
        """
        try:
            # Tokenç»Ÿè®¡
            total_prompt_tokens = 0
            total_completion_tokens = 0
            total_tokens = 0
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = self.prompt_manager.get_prompt('system_prompt')
            
            # æ„å»º contents åˆ—è¡¨ï¼ˆä½¿ç”¨ types.Content æ ¼å¼ï¼‰
            contents = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            if system_prompt:
                contents.append(types.Content(
                    role="user",
                    parts=[types.Part(text=system_prompt)]
                ))
            
            # æ·»åŠ å¯¹è¯å†å²
            if conversation_history:
                for msg in conversation_history:
                    role = msg.get('role', '')
                    content = msg.get('content', '')
                    
                    # è½¬æ¢è§’è‰²åç§°
                    genai_role = "model" if role == "assistant" else "user"
                    
                    if content:
                        contents.append(types.Content(
                            role=genai_role,
                            parts=[types.Part(text=content)]
                        ))
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            contents.append(types.Content(
                role="user",
                parts=[types.Part(text=user_message)]
            ))
            
            # é…ç½®å·¥å…·
            tools = []
            
            # æ³¨æ„ï¼šGoogle Search å’Œ Function Calling ä¸èƒ½åŒæ—¶ä½¿ç”¨ï¼ˆAPI é™åˆ¶ï¼‰
            # è§£å†³æ–¹æ¡ˆï¼šå°† Google Search æ”¹ä¸ºä¸€ä¸ª Function Calling å‡½æ•°
            
            # æ·»åŠ  Function Calling å·¥å…·ï¼ˆåŒ…å«å¾…åŠç®¡ç†å’Œæœç´¢ï¼‰
            function_declarations = self._convert_openai_tools_to_genai(TOOLS_SCHEMA)
            if function_declarations:
                function_tool = types.Tool(function_declarations=function_declarations)
                tools.append(function_tool)
                print(f"âœ… å·²æ·»åŠ  {len(function_declarations)} ä¸ªå‡½æ•°è°ƒç”¨å·¥å…·ï¼ˆåŒ…å«æœç´¢åŠŸèƒ½ï¼‰")
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            generate_config = types.GenerateContentConfig(
                temperature=self.temperature,
                max_output_tokens=self.max_tokens,
                tools=tools if tools else None
            )
            
            # æ·»åŠ æ€è€ƒé…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.thinking_budget is not None:
                generate_config.thinking_config = types.ThinkingConfig(
                    thinking_budget=self.thinking_budget,
                    include_thoughts=self.include_thoughts
                )
            
            # åˆ›å»ºå·¥å…·å®ä¾‹ï¼ˆç”¨äºæ‰§è¡Œå‡½æ•°è°ƒç”¨ï¼‰
            llm_tools = LLMTools(
                self.todo_service, 
                user_id,
                search_client=self.search_client,  # ä¼ é€’ç‹¬ç«‹çš„æœç´¢å®¢æˆ·ç«¯
                search_model=self.search_model,  # ä¼ é€’æœç´¢æ¨¡å‹åç§°
                search_temperature=self.search_temperature,  # ä¼ é€’æœç´¢æ¸©åº¦å‚æ•°
                transaction_service=self.transaction_service  # ä¼ é€’è®°è´¦æœåŠ¡
            )
            
            # è®°å½•æ‰€æœ‰è°ƒç”¨çš„å·¥å…·ï¼ˆç”¨äºåœ¨å›å¤æœ«å°¾æ·»åŠ æ ‡è®°ï¼‰
            called_tools = []
            
            # æ”¯æŒå¤šè½®å‡½æ•°è°ƒç”¨ï¼ˆæœ€å¤š5è½®ï¼‰
            max_iterations = 5
            iteration_count = 0
            
            print(f"è°ƒç”¨ Gemini APIï¼Œæ¨¡å‹: {self.model}ï¼ŒFunction Calling: {len(function_declarations) > 0}")
            
            for iteration in range(max_iterations):
                iteration_count += 1
                
                # è°ƒç”¨ Genai API
                response = self.genai_client.models.generate_content(
                    model=self.model,
                    contents=contents,
                    config=generate_config
                )
                
                # ç»Ÿè®¡ token
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    usage = response.usage_metadata
                    if hasattr(usage, 'prompt_token_count') and usage.prompt_token_count is not None:
                        total_prompt_tokens += usage.prompt_token_count
                    if hasattr(usage, 'candidates_token_count') and usage.candidates_token_count is not None:
                        total_completion_tokens += usage.candidates_token_count
                    if hasattr(usage, 'total_token_count') and usage.total_token_count is not None:
                        total_tokens += usage.total_token_count
                    
                    # å®‰å…¨åœ°è·å–tokenè®¡æ•°ç”¨äºæ—¥å¿—
                    prompt_tokens = usage.prompt_token_count if hasattr(usage, 'prompt_token_count') else 0
                    completion_tokens = usage.candidates_token_count if hasattr(usage, 'candidates_token_count') else 0
                    print(f"ç¬¬{iteration + 1}è½®è°ƒç”¨ - è¾“å…¥token: {prompt_tokens}, è¾“å‡ºtoken: {completion_tokens}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å‡½æ•°è°ƒç”¨
                if response.candidates and response.candidates[0].content.parts:
                    has_function_call = False
                    
                    # å°†æ¨¡å‹å“åº”æ·»åŠ åˆ°å¯¹è¯å†å²
                    contents.append(response.candidates[0].content)
                    
                    # å¤„ç†æ¯ä¸ª part
                    for part in response.candidates[0].content.parts:
                        if hasattr(part, 'function_call') and part.function_call:
                            has_function_call = True
                            function_call = part.function_call
                            
                            print(f"ğŸ”§ æ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨: {function_call.name}({dict(function_call.args)})")
                            
                            # è®°å½•å·¥å…·è°ƒç”¨ï¼ˆç”¨äºæœ€ç»ˆæ˜¾ç¤ºï¼‰
                            tool_name_map = {
                                'search_web': 'æœç´¢å·¥å…·',
                                'create_todo': 'å¾…åŠåˆ›å»º',
                                'get_todo_list': 'å¾…åŠæŸ¥è¯¢',
                                'complete_todo': 'å¾…åŠå®Œæˆ',
                                'delete_todo': 'å¾…åŠåˆ é™¤',
                                'update_todo': 'å¾…åŠæ›´æ–°',
                                'record_expense': 'è®°å½•æ”¯å‡º',
                                'record_income': 'è®°å½•æ”¶å…¥',
                                'adjust_balance': 'èµ„é‡‘çŸ«æ­£',
                                'get_balance': 'æŸ¥è¯¢ä½™é¢',
                                'get_transactions': 'æŸ¥è¯¢è®°è´¦',
                                'get_financial_summary': 'æ”¶æ”¯æ±‡æ€»'
                            }
                            tool_display_name = tool_name_map.get(function_call.name, function_call.name)
                            if tool_display_name not in called_tools:
                                called_tools.append(tool_display_name)
                            
                            # æ‰§è¡Œå‡½æ•°
                            function_result = llm_tools.execute_tool_call(
                                function_call.name,
                                dict(function_call.args)
                            )
                            
                            print(f"âœ… å‡½æ•°æ‰§è¡Œç»“æœ: {function_result}")
                            # åˆ›å»ºå‡½æ•°å“åº” part
                            function_response_part = types.Part.from_function_response(
                                name=function_call.name,
                                response={"result": function_result}
                            )
                            
                            # æ·»åŠ å‡½æ•°ç»“æœåˆ°å¯¹è¯å†å²
                            contents.append(types.Content(
                                role="user",
                                parts=[function_response_part]
                            ))
                    
                    # å¦‚æœæ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œè¯´æ˜æ¨¡å‹å·²ç»ç”Ÿæˆäº†æœ€ç»ˆå›ç­”
                    if not has_function_call:
                        # æå–å›ç­”æ–‡æœ¬å’Œæ€è€ƒæ€»ç»“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        answer_text = ""
                        thought_summary = ""
                        
                        for part in response.candidates[0].content.parts:
                            if hasattr(part, 'text') and part.text:
                                if hasattr(part, 'thought') and part.thought:
                                    # è¿™æ˜¯æ€è€ƒæ€»ç»“
                                    thought_summary += part.text
                                else:
                                    # è¿™æ˜¯æœ€ç»ˆå›ç­”
                                    answer_text += part.text
                        
                        # å¦‚æœå¯ç”¨äº†æ€è€ƒæ€»ç»“è¾“å‡ºï¼Œæ‰“å°æ€è€ƒè¿‡ç¨‹
                        if thought_summary and self.include_thoughts:
                            print(f"ğŸ’­ æ€è€ƒæ€»ç»“:\n{thought_summary}\n")
                        
                        # å¦‚æœæ²¡æœ‰æå–åˆ°æ–‡æœ¬ï¼Œä½¿ç”¨é»˜è®¤çš„ response.text
                        if not answer_text:
                            answer_text = response.text if hasattr(response, 'text') else "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ã€‚"
                        
                        # åœ¨å›å¤æœ«å°¾æ·»åŠ å·¥å…·è°ƒç”¨æ ‡è®°
                        if called_tools:
                            tools_text = "ã€".join(called_tools)
                            answer_text += f"\n\n[å·²è°ƒç”¨{tools_text}]"
                        
                        break
                    
                    # ç»§ç»­ä¸‹ä¸€è½®ï¼ˆè®©æ¨¡å‹åŸºäºå‡½æ•°ç»“æœç”Ÿæˆå›ç­”ï¼‰
                else:
                    # æ²¡æœ‰æœ‰æ•ˆå“åº”
                    answer_text = response.text if hasattr(response, 'text') else "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ã€‚"
                    break
            else:
                # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
                print(f"âš ï¸ è­¦å‘Šï¼šè¾¾åˆ°æœ€å¤§å‡½æ•°è°ƒç”¨è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå¼ºåˆ¶è¿”å›")
                answer_text = response.text if hasattr(response, 'text') else "æŠ±æ­‰ï¼Œå¤„ç†æ—¶é—´è¿‡é•¿ã€‚"
            
            # æ‰“å° Token ç»Ÿè®¡
            print(f"=" * 50)
            print(f"æœ¬æ¬¡å¯¹è¯Tokenç»Ÿè®¡:")
            print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
            print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
            
            # å¦‚æœæœ‰æ€è€ƒ tokenï¼Œå•ç‹¬æ˜¾ç¤º
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                if hasattr(response.usage_metadata, 'thoughts_token_count'):
                    thoughts_tokens = response.usage_metadata.thoughts_token_count
                    if thoughts_tokens is not None and thoughts_tokens > 0:
                        print(f"  æ€è€ƒtoken: {thoughts_tokens}")
            
            print(f"  æ€»è®¡token: {total_tokens}")
            print(f"  å‡½æ•°è°ƒç”¨è½®æ¬¡: {iteration_count}")
            print(f"=" * 50)
            
            # ç¡®ä¿å·¥å…·è°ƒç”¨æ ‡è®°è¢«æ·»åŠ ï¼ˆé˜²æ­¢æŸäº›å¼‚å¸¸é€€å‡ºæƒ…å†µï¼‰
            if called_tools and "[å·²è°ƒç”¨" not in answer_text:
                tools_text = "ã€".join(called_tools)
                answer_text += f"\n\n[å·²è°ƒç”¨{tools_text}]"
            
            return answer_text
            
        except Exception as e:
            print(f"Gemini API è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def chat_with_function_calling(self, user_id, user_message, conversation_history=None):
        """
        ä¸å¤§æ¨¡å‹å¯¹è¯ï¼Œæ”¯æŒFunction Calling
        
        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤§æ¨¡å‹çš„å›å¤æ–‡æœ¬
        """
        # å¦‚æœä½¿ç”¨ Google Genai SDKï¼Œè°ƒç”¨ä¸“é—¨çš„æ–¹æ³•
        if self.use_genai_sdk:
            return self._chat_with_genai_sdk(user_id, user_message, conversation_history)
        
        # ä»¥ä¸‹æ˜¯åŸæœ‰çš„ OpenAI SDK å®ç°
        try:
            # Tokenç»Ÿè®¡
            total_prompt_tokens = 0
            total_completion_tokens = 0
            total_tokens = 0
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            system_prompt = self.prompt_manager.get_prompt('system_prompt')
            if system_prompt:
                messages.append({
                    "role": "system",
                    "content": system_prompt
                })
            
            # æ·»åŠ å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
            if conversation_history:
                messages.extend(conversation_history)
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({
                "role": "user",
                "content": user_message
            })
            
            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=TOOLS_SCHEMA,
                tool_choice="auto",
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            # ç»Ÿè®¡ç¬¬ä¸€æ¬¡è°ƒç”¨çš„token
            if hasattr(response, 'usage') and response.usage:
                if response.usage.prompt_tokens is not None:
                    total_prompt_tokens += response.usage.prompt_tokens
                if response.usage.completion_tokens is not None:
                    total_completion_tokens += response.usage.completion_tokens
                if response.usage.total_tokens is not None:
                    total_tokens += response.usage.total_tokens
                print(f"ç¬¬1è½®è°ƒç”¨ - è¾“å…¥token: {response.usage.prompt_tokens or 0}, è¾“å‡ºtoken: {response.usage.completion_tokens or 0}")
            
            # åˆ›å»ºå·¥å…·å®ä¾‹ï¼ˆç”¨äºæ‰§è¡Œå‡½æ•°è°ƒç”¨ï¼‰
            llm_tools = LLMTools(
                self.todo_service, 
                user_id,
                search_client=self.search_client,  # ä¼ é€’ç‹¬ç«‹çš„æœç´¢å®¢æˆ·ç«¯
                search_model=self.search_model,  # ä¼ é€’æœç´¢æ¨¡å‹åç§°
                search_temperature=self.search_temperature,  # ä¼ é€’æœç´¢æ¸©åº¦å‚æ•°
                transaction_service=self.transaction_service  # ä¼ é€’è®°è´¦æœåŠ¡
            )
            
            # è®°å½•æ‰€æœ‰è°ƒç”¨çš„å·¥å…·ï¼ˆç”¨äºåœ¨å›å¤æœ«å°¾æ·»åŠ æ ‡è®°ï¼‰
            called_tools = []
            tool_name_map = {
                'search_web': 'æœç´¢å·¥å…·',
                'create_todo': 'å¾…åŠåˆ›å»º',
                'get_todo_list': 'å¾…åŠæŸ¥è¯¢',
                'complete_todo': 'å¾…åŠå®Œæˆ',
                'delete_todo': 'å¾…åŠåˆ é™¤',
                'update_todo': 'å¾…åŠæ›´æ–°',
                'record_expense': 'è®°å½•æ”¯å‡º',
                'record_income': 'è®°å½•æ”¶å…¥',
                'adjust_balance': 'èµ„é‡‘çŸ«æ­£',
                'get_balance': 'æŸ¥è¯¢ä½™é¢',
                'get_transactions': 'æŸ¥è¯¢è®°è´¦',
                'get_financial_summary': 'æ”¶æ”¯æ±‡æ€»'
            }
            
            # æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæœ€å¤š5è½®ï¼Œé˜²æ­¢æ— é™å¾ªç¯ï¼‰
            max_iterations = 5
            for iteration in range(max_iterations):
                # å¤„ç†å“åº”
                assistant_message = response.choices[0].message
                
                # å¦‚æœå¤§æ¨¡å‹éœ€è¦è°ƒç”¨å‡½æ•°
                if assistant_message.tool_calls:
                    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                    messages.append(assistant_message)
                    
                    # æ‰§è¡Œæ‰€æœ‰å‡½æ•°è°ƒç”¨
                    for tool_call in assistant_message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)
                        
                        print(f"æ‰§è¡Œå‡½æ•°è°ƒç”¨: {function_name}({function_args})")
                        
                        # è®°å½•å·¥å…·è°ƒç”¨ï¼ˆç”¨äºæœ€ç»ˆæ˜¾ç¤ºï¼‰
                        tool_display_name = tool_name_map.get(function_name, function_name)
                        if tool_display_name not in called_tools:
                            called_tools.append(tool_display_name)
                        
                        # æ‰§è¡Œå‡½æ•°
                        function_result = llm_tools.execute_tool_call(function_name, function_args)
                        
                        # æ·»åŠ å‡½æ•°ç»“æœåˆ°æ¶ˆæ¯åˆ—è¡¨
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps(function_result, ensure_ascii=False)
                        })
                    
                    # å†æ¬¡è°ƒç”¨å¤§æ¨¡å‹ï¼Œè®©å®ƒåŸºäºå‡½æ•°ç»“æœç”Ÿæˆå›å¤æˆ–ç»§ç»­è°ƒç”¨å·¥å…·
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        tools=TOOLS_SCHEMA,
                        tool_choice="auto",
                        temperature=self.temperature,
                        max_tokens=self.max_tokens
                    )
                    
                    # ç»Ÿè®¡åç»­è°ƒç”¨çš„token
                    if hasattr(response, 'usage') and response.usage:
                        if response.usage.prompt_tokens is not None:
                            total_prompt_tokens += response.usage.prompt_tokens
                        if response.usage.completion_tokens is not None:
                            total_completion_tokens += response.usage.completion_tokens
                        if response.usage.total_tokens is not None:
                            total_tokens += response.usage.total_tokens
                        print(f"ç¬¬{iteration + 2}è½®è°ƒç”¨ - è¾“å…¥token: {response.usage.prompt_tokens or 0}, è¾“å‡ºtoken: {response.usage.completion_tokens or 0}")
                    
                    # ç»§ç»­å¾ªç¯ï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ–°çš„å·¥å…·è°ƒç”¨
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨äº†ï¼Œè¿”å›æœ€ç»ˆå›å¤
                    final_content = assistant_message.content
                    
                    # åœ¨å›å¤æœ«å°¾æ·»åŠ å·¥å…·è°ƒç”¨æ ‡è®°
                    if called_tools:
                        tools_text = "ã€".join(called_tools)
                        final_content += f"\n\n[å·²è°ƒç”¨{tools_text}]"
                    
                    print(f"=" * 50)
                    print(f"æœ¬æ¬¡å¯¹è¯Tokenç»Ÿè®¡:")
                    print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
                    print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
                    print(f"  æ€»è®¡token: {total_tokens}")
                    print(f"=" * 50)
                    return final_content
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè¿”å›æœ€åçš„å›å¤
            print(f"è­¦å‘Šï¼šè¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå¼ºåˆ¶è¿”å›")
            
            final_content = response.choices[0].message.content
            # åœ¨å›å¤æœ«å°¾æ·»åŠ å·¥å…·è°ƒç”¨æ ‡è®°
            if called_tools and "[å·²è°ƒç”¨" not in final_content:
                tools_text = "ã€".join(called_tools)
                final_content += f"\n\n[å·²è°ƒç”¨{tools_text}]"
            
            print(f"=" * 50)
            print(f"æœ¬æ¬¡å¯¹è¯Tokenç»Ÿè®¡:")
            print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
            print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
            print(f"  æ€»è®¡token: {total_tokens}")
            print(f"=" * 50)
            return final_content
            
        except Exception as e:
            print(f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def chat_with_images(self, user_id, user_message, image_paths):
        """
        ä¸å¤§æ¨¡å‹å¯¹è¯ï¼Œæ”¯æŒå‘é€å›¾ç‰‡å’ŒFunction Calling
        
        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            image_paths: å›¾ç‰‡è·¯å¾„åˆ—è¡¨
            
        Returns:
            å¤§æ¨¡å‹çš„å›å¤æ–‡æœ¬
        """
        # å¦‚æœä¸ä½¿ç”¨ Google Genai SDKï¼Œè¿”å›é”™è¯¯æç¤º
        if not self.use_genai_sdk or not GENAI_AVAILABLE:
            return "æŠ±æ­‰ï¼Œå½“å‰é…ç½®çš„æ¨¡å‹ä¸æ”¯æŒå›¾ç‰‡å¤„ç†åŠŸèƒ½ã€‚è¯·åˆ‡æ¢åˆ° Gemini æ¨¡å‹ã€‚"
        
        try:
            print(f"å¤„ç†å›¾ç‰‡æ¶ˆæ¯ - ç”¨æˆ·: {user_id}, å›¾ç‰‡æ•°é‡: {len(image_paths)}")
            
            # Tokenç»Ÿè®¡
            total_prompt_tokens = 0
            total_completion_tokens = 0
            total_tokens = 0
            
            # è·å–å½“å‰æ—¶é—´ï¼ˆåŒ—äº¬æ—¶é—´ï¼‰
            beijing_tz = pytz.timezone('Asia/Shanghai')
            current_time = datetime.now(beijing_tz)
            weekday_names = ['æ˜ŸæœŸä¸€', 'æ˜ŸæœŸäºŒ', 'æ˜ŸæœŸä¸‰', 'æ˜ŸæœŸå››', 'æ˜ŸæœŸäº”', 'æ˜ŸæœŸå…­', 'æ˜ŸæœŸæ—¥']
            current_weekday = weekday_names[current_time.weekday()]
            
            # è·å–å›¾ç‰‡ç†è§£ä¸“ç”¨ç³»ç»Ÿæç¤ºè¯
            system_prompt = self.prompt_manager.get_prompt(
                'image_system_prompt',
                current_time=current_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M'),
                current_weekday=current_weekday
            )
            
            # æ„å»º contents åˆ—è¡¨
            contents = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            if system_prompt:
                contents.append(types.Content(
                    role="user",
                    parts=[types.Part(text=system_prompt)]
                ))
                print(f"âœ… å·²æ·»åŠ å›¾ç‰‡ç†è§£ç³»ç»Ÿæç¤ºè¯")
            
            # å‡†å¤‡ç”¨æˆ·æ¶ˆæ¯çš„partsï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡ï¼‰
            user_parts = [types.Part(text=user_message)]
            
            # æ·»åŠ å›¾ç‰‡åˆ°user_parts
            for image_path in image_paths:
                try:
                    # è¯»å–å›¾ç‰‡æ–‡ä»¶
                    with open(image_path, 'rb') as f:
                        image_bytes = f.read()
                    
                    # åˆ¤æ–­å›¾ç‰‡æ ¼å¼
                    mime_type = 'image/jpeg'  # é»˜è®¤
                    if image_path.lower().endswith('.png'):
                        mime_type = 'image/png'
                    elif image_path.lower().endswith('.gif'):
                        mime_type = 'image/gif'
                    elif image_path.lower().endswith('.webp'):
                        mime_type = 'image/webp'
                    
                    # åˆ›å»ºå›¾ç‰‡ Part
                    image_part = types.Part.from_bytes(
                        data=image_bytes,
                        mime_type=mime_type
                    )
                    
                    user_parts.append(image_part)
                    print(f"âœ… å·²æ·»åŠ å›¾ç‰‡: {image_path} ({mime_type})")
                    
                except Exception as e:
                    print(f"âŒ è¯»å–å›¾ç‰‡å¤±è´¥ ({image_path}): {e}")
                    continue
            
            # å°†ç”¨æˆ·æ¶ˆæ¯ï¼ˆæ–‡æœ¬+å›¾ç‰‡ï¼‰ä½œä¸ºä¸€ä¸ªContentæ·»åŠ 
            contents.append(types.Content(
                role="user",
                parts=user_parts
            ))
            
            # é…ç½®å·¥å…·
            tools = []
            
            # æ·»åŠ  Function Calling å·¥å…·ï¼ˆåŒ…å«å¾…åŠç®¡ç†å’Œæœç´¢ï¼‰
            function_declarations = self._convert_openai_tools_to_genai(TOOLS_SCHEMA)
            if function_declarations:
                function_tool = types.Tool(function_declarations=function_declarations)
                tools.append(function_tool)
                print(f"âœ… å·²æ·»åŠ  {len(function_declarations)} ä¸ªå‡½æ•°è°ƒç”¨å·¥å…·ï¼ˆåŒ…å«æœç´¢åŠŸèƒ½ï¼‰")
            
            # é…ç½®ç”Ÿæˆå‚æ•°ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨å’Œå›¾ç‰‡ç†è§£ï¼‰
            generate_config = types.GenerateContentConfig(
                temperature=self.temperature,
                max_output_tokens=self.max_tokens,
                tools=tools if tools else None
            )
            
            # æ·»åŠ æ€è€ƒé…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.thinking_budget is not None:
                generate_config.thinking_config = types.ThinkingConfig(
                    thinking_budget=self.thinking_budget,
                    include_thoughts=self.include_thoughts
                )
            
            # åˆ›å»ºå·¥å…·å®ä¾‹ï¼ˆç”¨äºæ‰§è¡Œå‡½æ•°è°ƒç”¨ï¼‰
            llm_tools = LLMTools(
                self.todo_service, 
                user_id,
                search_client=self.search_client,
                search_model=self.search_model,
                search_temperature=self.search_temperature,
                transaction_service=self.transaction_service
            )
            
            # è®°å½•æ‰€æœ‰è°ƒç”¨çš„å·¥å…·ï¼ˆç”¨äºåœ¨å›å¤æœ«å°¾æ·»åŠ æ ‡è®°ï¼‰
            called_tools = []
            
            # æ”¯æŒå¤šè½®å‡½æ•°è°ƒç”¨ï¼ˆæœ€å¤š5è½®ï¼‰
            max_iterations = 5
            iteration_count = 0
            
            print(f"è°ƒç”¨ Gemini API è¿›è¡Œå›¾ç‰‡ç†è§£ï¼Œæ¨¡å‹: {self.model}ï¼ŒFunction Calling: {len(function_declarations) > 0}")
            
            for iteration in range(max_iterations):
                iteration_count += 1
                
                # è°ƒç”¨ Gemini API
                response = self.genai_client.models.generate_content(
                    model=self.model,
                    contents=contents,
                    config=generate_config
                )
                
                # ç»Ÿè®¡ token
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    usage = response.usage_metadata
                    if hasattr(usage, 'prompt_token_count') and usage.prompt_token_count is not None:
                        total_prompt_tokens += usage.prompt_token_count
                    if hasattr(usage, 'candidates_token_count') and usage.candidates_token_count is not None:
                        total_completion_tokens += usage.candidates_token_count
                    if hasattr(usage, 'total_token_count') and usage.total_token_count is not None:
                        total_tokens += usage.total_token_count
                    
                    # å®‰å…¨åœ°è·å–tokenè®¡æ•°ç”¨äºæ—¥å¿—
                    prompt_tokens = usage.prompt_token_count if hasattr(usage, 'prompt_token_count') else 0
                    completion_tokens = usage.candidates_token_count if hasattr(usage, 'candidates_token_count') else 0
                    print(f"ç¬¬{iteration + 1}è½®è°ƒç”¨ - è¾“å…¥token: {prompt_tokens}, è¾“å‡ºtoken: {completion_tokens}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å‡½æ•°è°ƒç”¨
                if response.candidates and response.candidates[0].content.parts:
                    has_function_call = False
                    
                    # å°†æ¨¡å‹å“åº”æ·»åŠ åˆ°å¯¹è¯å†å²
                    contents.append(response.candidates[0].content)
                    
                    # å¤„ç†æ¯ä¸ª part
                    for part in response.candidates[0].content.parts:
                        if hasattr(part, 'function_call') and part.function_call:
                            has_function_call = True
                            function_call = part.function_call
                            
                            print(f"ğŸ”§ æ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨: {function_call.name}({dict(function_call.args)})")
                            
                            # è®°å½•å·¥å…·è°ƒç”¨ï¼ˆç”¨äºæœ€ç»ˆæ˜¾ç¤ºï¼‰
                            tool_name_map = {
                                'search_web': 'æœç´¢å·¥å…·',
                                'create_todo': 'å¾…åŠåˆ›å»º',
                                'get_todo_list': 'å¾…åŠæŸ¥è¯¢',
                                'complete_todo': 'å¾…åŠå®Œæˆ',
                                'delete_todo': 'å¾…åŠåˆ é™¤',
                                'update_todo': 'å¾…åŠæ›´æ–°',
                                'record_expense': 'è®°å½•æ”¯å‡º',
                                'record_income': 'è®°å½•æ”¶å…¥',
                                'adjust_balance': 'èµ„é‡‘çŸ«æ­£',
                                'get_balance': 'æŸ¥è¯¢ä½™é¢',
                                'get_transactions': 'æŸ¥è¯¢è®°è´¦',
                                'get_financial_summary': 'æ”¶æ”¯æ±‡æ€»'
                            }
                            tool_display_name = tool_name_map.get(function_call.name, function_call.name)
                            if tool_display_name not in called_tools:
                                called_tools.append(tool_display_name)
                            
                            # æ‰§è¡Œå‡½æ•°
                            function_result = llm_tools.execute_tool_call(
                                function_call.name,
                                dict(function_call.args)
                            )
                            
                            print(f"âœ… å‡½æ•°æ‰§è¡Œç»“æœ: {function_result}")
                            
                            # åˆ›å»ºå‡½æ•°å“åº” part
                            function_response_part = types.Part.from_function_response(
                                name=function_call.name,
                                response={"result": function_result}
                            )
                            
                            # æ·»åŠ å‡½æ•°ç»“æœåˆ°å¯¹è¯å†å²
                            contents.append(types.Content(
                                role="user",
                                parts=[function_response_part]
                            ))
                    
                    # å¦‚æœæ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œè¯´æ˜æ¨¡å‹å·²ç»ç”Ÿæˆäº†æœ€ç»ˆå›ç­”
                    if not has_function_call:
                        # æå–å›ç­”æ–‡æœ¬å’Œæ€è€ƒæ€»ç»“ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        answer_text = ""
                        thought_summary = ""
                        
                        for part in response.candidates[0].content.parts:
                            if hasattr(part, 'text') and part.text:
                                if hasattr(part, 'thought') and part.thought:
                                    # è¿™æ˜¯æ€è€ƒæ€»ç»“
                                    thought_summary += part.text
                                else:
                                    # è¿™æ˜¯æœ€ç»ˆå›ç­”
                                    answer_text += part.text
                        
                        # å¦‚æœå¯ç”¨äº†æ€è€ƒæ€»ç»“è¾“å‡ºï¼Œæ‰“å°æ€è€ƒè¿‡ç¨‹
                        if thought_summary and self.include_thoughts:
                            print(f"ğŸ’­ æ€è€ƒæ€»ç»“:\n{thought_summary}\n")
                        
                        # å¦‚æœæ²¡æœ‰æå–åˆ°æ–‡æœ¬ï¼Œä½¿ç”¨é»˜è®¤çš„ response.text
                        if not answer_text:
                            answer_text = response.text if hasattr(response, 'text') else "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç†è§£è¿™äº›å›¾ç‰‡ã€‚"
                        
                        # åœ¨å›å¤æœ«å°¾æ·»åŠ å·¥å…·è°ƒç”¨æ ‡è®°
                        if called_tools:
                            tools_text = "ã€".join(called_tools)
                            answer_text += f"\n\n[å·²è°ƒç”¨{tools_text}]"
                        
                        break
                    
                    # ç»§ç»­ä¸‹ä¸€è½®ï¼ˆè®©æ¨¡å‹åŸºäºå‡½æ•°ç»“æœç”Ÿæˆå›ç­”ï¼‰
                else:
                    # æ²¡æœ‰æœ‰æ•ˆå“åº”
                    answer_text = response.text if hasattr(response, 'text') else "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ã€‚"
                    break
            else:
                # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
                print(f"âš ï¸ è­¦å‘Šï¼šè¾¾åˆ°æœ€å¤§å‡½æ•°è°ƒç”¨è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå¼ºåˆ¶è¿”å›")
                answer_text = response.text if hasattr(response, 'text') else "æŠ±æ­‰ï¼Œå¤„ç†æ—¶é—´è¿‡é•¿ã€‚"
            
            # æ‰“å° Token ç»Ÿè®¡
            print(f"=" * 50)
            print(f"å›¾ç‰‡ç†è§£Tokenç»Ÿè®¡:")
            print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
            print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
            
            # å¦‚æœæœ‰æ€è€ƒ tokenï¼Œå•ç‹¬æ˜¾ç¤º
            if hasattr(response, 'usage_metadata') and response.usage_metadata:
                if hasattr(response.usage_metadata, 'thoughts_token_count'):
                    thoughts_tokens = response.usage_metadata.thoughts_token_count
                    if thoughts_tokens is not None and thoughts_tokens > 0:
                        print(f"  æ€è€ƒtoken: {thoughts_tokens}")
            
            print(f"  æ€»è®¡token: {total_tokens}")
            print(f"  å‡½æ•°è°ƒç”¨è½®æ¬¡: {iteration_count}")
            print(f"=" * 50)
            
            # ç¡®ä¿å·¥å…·è°ƒç”¨æ ‡è®°è¢«æ·»åŠ ï¼ˆé˜²æ­¢æŸäº›å¼‚å¸¸é€€å‡ºæƒ…å†µï¼‰
            if called_tools and "[å·²è°ƒç”¨" not in answer_text:
                tools_text = "ã€".join(called_tools)
                answer_text += f"\n\n[å·²è°ƒç”¨{tools_text}]"
            
            return answer_text
            
        except Exception as e:
            print(f"å›¾ç‰‡ç†è§£å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"æŠ±æ­‰ï¼Œå¤„ç†å›¾ç‰‡æ—¶å‡ºç°äº†é—®é¢˜ï¼š{str(e)}"
    
    def generate_daily_plan(self, user_id):
        """
        ç”Ÿæˆæ¯æ—¥ä»»åŠ¡è§„åˆ’
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            è§„åˆ’æ–‡æœ¬
        """
        try:
            # è·å–æ˜¨å¤©å®Œæˆçš„ä»»åŠ¡
            yesterday_todos = self.todo_service.get_yesterday_completed_todos(user_id)
            yesterday_summary = "\n".join([
                f"- {todo.content}" for todo in yesterday_todos
            ]) if yesterday_todos else "æ— "
            
            # è·å–ä»Šå¤©çš„å¾…åŠä»»åŠ¡
            today_todos = self.todo_service.get_today_todos(user_id)
            today_tasks = "\n".join([
                f"- [{todo.priority}] {todo.content}" for todo in today_todos
            ]) if today_todos else "æ— "
            
            # è·å–è§„åˆ’æç¤ºè¯
            prompt = self.prompt_manager.get_prompt(
                'daily_planning_prompt',
                yesterday_summary=yesterday_summary,
                today_tasks=today_tasks
            )
            
            # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆè§„åˆ’
            messages = [
                {"role": "system", "content": self.prompt_manager.get_prompt('system_prompt')},
                {"role": "user", "content": prompt}
            ]
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            # ç»Ÿè®¡tokenä½¿ç”¨
            if hasattr(response, 'usage') and response.usage:
                print(f"=" * 50)
                print(f"æ¯æ—¥è§„åˆ’Tokenç»Ÿè®¡:")
                print(f"  è¾“å…¥token: {response.usage.prompt_tokens}")
                print(f"  è¾“å‡ºtoken: {response.usage.completion_tokens}")
                print(f"  æ€»è®¡token: {response.usage.total_tokens}")
                print(f"=" * 50)
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"ç”Ÿæˆæ¯æ—¥è§„åˆ’å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆä»Šæ—¥è§„åˆ’ã€‚"

