# -*- coding: utf-8 -*-
"""
å¤§æ¨¡å‹æœåŠ¡
å¤„ç†ä¸å¤§è¯­è¨€æ¨¡å‹çš„äº¤äº’ï¼ŒåŒ…æ‹¬Function Calling
"""
import json
from openai import OpenAI
from app.utils.llm_tools import TOOLS_SCHEMA, LLMTools

# Google Genai SDK (ç”¨äºGemini Google Search)
try:
    from google import genai
    from google.genai import types
    GENAI_AVAILABLE = True
except ImportError:
    GENAI_AVAILABLE = False
    print("è­¦å‘Š: google-genai SDK æœªå®‰è£…ï¼ŒGemini Google Search åŠŸèƒ½å°†ä¸å¯ç”¨")


class LLMService:
    """å¤§æ¨¡å‹æœåŠ¡"""
    
    def __init__(self, config, prompt_manager, todo_service):
        """
        åˆå§‹åŒ–å¤§æ¨¡å‹æœåŠ¡
        
        Args:
            config: é…ç½®å¯¹è±¡
            prompt_manager: æç¤ºè¯ç®¡ç†å™¨
            todo_service: å¾…åŠäº‹é¡¹æœåŠ¡
        """
        self.config = config
        self.prompt_manager = prompt_manager
        self.todo_service = todo_service
        
        # è·å–å½“å‰æ¨¡å‹é…ç½®
        current_llm = config['CURRENT_LLM']
        llm_config = config['LLM_MODELS'][current_llm]
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ Google Genai SDK
        self.use_genai_sdk = llm_config.get('use_genai_sdk', False)
        self.use_google_search = llm_config.get('use_google_search', False)
        
        # åˆå§‹åŒ–ä¸»æ¨¡å‹å®¢æˆ·ç«¯
        if self.use_genai_sdk and GENAI_AVAILABLE:
            # ä½¿ç”¨ Google Genai SDK
            self.genai_client = genai.Client(api_key=config['LLM_API_KEY'])
            self.client = None
            print(f"âœ… ä½¿ç”¨ Google Genai SDK - ä¸»æ¨¡å‹: {config['LLM_MODEL']}")
        else:
            # ä½¿ç”¨ OpenAI SDKï¼ˆå…¼å®¹å¤šç§APIï¼‰
            self.client = OpenAI(
                api_key=config['LLM_API_KEY'],
                base_url=config['LLM_API_BASE']
            )
            self.genai_client = None
            print(f"âœ… ä½¿ç”¨ OpenAI å…¼å®¹ SDK - ä¸»æ¨¡å‹: {config['LLM_MODEL']}")
        
        self.model = config['LLM_MODEL']
        self.temperature = config['LLM_TEMPERATURE']
        self.max_tokens = config['LLM_MAX_TOKENS']
        
        # åˆå§‹åŒ–ç‹¬ç«‹çš„æœç´¢å®¢æˆ·ç«¯ï¼ˆå¦‚æœä¸»æ¨¡å‹å¯ç”¨äº†æœç´¢åŠŸèƒ½ï¼‰
        self.search_client = None
        self.search_model = None
        self.search_temperature = None
        
        if self.use_google_search and GENAI_AVAILABLE:
            # ä»é…ç½®ä¸­è·å–æœç´¢æ¨¡å‹å‚æ•°
            search_config = config.get('SEARCH_MODEL_CONFIG', {})
            search_api_key = search_config.get('api_key')
            
            if search_api_key:
                # åˆå§‹åŒ–æœç´¢ä¸“ç”¨å®¢æˆ·ç«¯ï¼ˆæ‰€æœ‰å‚æ•°ä»é…ç½®è¯»å–ï¼‰
                self.search_client = genai.Client(api_key=search_api_key)
                self.search_model = search_config.get('model', 'gemini-2.0-flash-exp')
                self.search_temperature = search_config.get('temperature', 0.7)  # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ï¼Œé»˜è®¤0.7
                print(f"ğŸ” å·²å¯ç”¨ç½‘ç»œæœç´¢åŠŸèƒ½ - æœç´¢æ¨¡å‹: {self.search_model}, æ¸©åº¦: {self.search_temperature}")
            else:
                print(f"âš ï¸ è­¦å‘Š: ä¸»æ¨¡å‹å·²å¼€å¯æœç´¢åŠŸèƒ½ï¼Œä½†æœªé…ç½® SEARCH_MODEL_CONFIG")
                self.use_google_search = False
        elif self.use_google_search and not GENAI_AVAILABLE:
            print(f"âš ï¸ è­¦å‘Š: ä¸»æ¨¡å‹å·²å¼€å¯æœç´¢åŠŸèƒ½ï¼Œä½† google-genai SDK æœªå®‰è£…")
            self.use_google_search = False
    
    def _convert_openai_tools_to_genai(self, openai_tools):
        """
        å°† OpenAI æ ¼å¼çš„å·¥å…·å®šä¹‰è½¬æ¢ä¸º Google Genai SDK æ ¼å¼
        
        Args:
            openai_tools: OpenAI æ ¼å¼çš„å·¥å…·åˆ—è¡¨
            
        Returns:
            Google Genai SDK æ ¼å¼çš„å‡½æ•°å£°æ˜åˆ—è¡¨
        """
        if not GENAI_AVAILABLE:
            return []
        
        function_declarations = []
        
        for tool in openai_tools:
            if tool.get('type') == 'function':
                func = tool['function']
                
                # è½¬æ¢ä¸º Genai æ ¼å¼
                function_declaration = {
                    'name': func['name'],
                    'description': func['description'],
                    'parameters': func['parameters']
                }
                
                function_declarations.append(function_declaration)
        
        return function_declarations
    
    def _chat_with_genai_sdk(self, user_id, user_message, conversation_history=None):
        """
        ä½¿ç”¨ Google Genai SDK è¿›è¡Œå¯¹è¯ï¼ˆæ”¯æŒ Google Search å’Œ Function Callingï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤§æ¨¡å‹çš„å›å¤æ–‡æœ¬
        """
        try:
            # Tokenç»Ÿè®¡
            total_prompt_tokens = 0
            total_completion_tokens = 0
            total_tokens = 0
            
            # æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = self.prompt_manager.get_prompt('system_prompt')
            
            # æ„å»º contents åˆ—è¡¨ï¼ˆä½¿ç”¨ types.Content æ ¼å¼ï¼‰
            contents = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            if system_prompt:
                contents.append(types.Content(
                    role="user",
                    parts=[types.Part(text=system_prompt)]
                ))
            
            # æ·»åŠ å¯¹è¯å†å²
            if conversation_history:
                for msg in conversation_history:
                    role = msg.get('role', '')
                    content = msg.get('content', '')
                    
                    # è½¬æ¢è§’è‰²åç§°
                    genai_role = "model" if role == "assistant" else "user"
                    
                    if content:
                        contents.append(types.Content(
                            role=genai_role,
                            parts=[types.Part(text=content)]
                        ))
            
            # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
            contents.append(types.Content(
                role="user",
                parts=[types.Part(text=user_message)]
            ))
            
            # é…ç½®å·¥å…·
            tools = []
            
            # æ³¨æ„ï¼šGoogle Search å’Œ Function Calling ä¸èƒ½åŒæ—¶ä½¿ç”¨ï¼ˆAPI é™åˆ¶ï¼‰
            # è§£å†³æ–¹æ¡ˆï¼šå°† Google Search æ”¹ä¸ºä¸€ä¸ª Function Calling å‡½æ•°
            
            # æ·»åŠ  Function Calling å·¥å…·ï¼ˆåŒ…å«å¾…åŠç®¡ç†å’Œæœç´¢ï¼‰
            function_declarations = self._convert_openai_tools_to_genai(TOOLS_SCHEMA)
            if function_declarations:
                function_tool = types.Tool(function_declarations=function_declarations)
                tools.append(function_tool)
                print(f"âœ… å·²æ·»åŠ  {len(function_declarations)} ä¸ªå‡½æ•°è°ƒç”¨å·¥å…·ï¼ˆåŒ…å«æœç´¢åŠŸèƒ½ï¼‰")
            
            # é…ç½®ç”Ÿæˆå‚æ•°
            config = types.GenerateContentConfig(
                temperature=self.temperature,
                max_output_tokens=self.max_tokens,
                tools=tools if tools else None
            )
            
            # åˆ›å»ºå·¥å…·å®ä¾‹ï¼ˆç”¨äºæ‰§è¡Œå‡½æ•°è°ƒç”¨ï¼‰
            llm_tools = LLMTools(
                self.todo_service, 
                user_id,
                search_client=self.search_client,  # ä¼ é€’ç‹¬ç«‹çš„æœç´¢å®¢æˆ·ç«¯
                search_model=self.search_model,  # ä¼ é€’æœç´¢æ¨¡å‹åç§°
                search_temperature=self.search_temperature  # ä¼ é€’æœç´¢æ¸©åº¦å‚æ•°
            )
            
            # æ”¯æŒå¤šè½®å‡½æ•°è°ƒç”¨ï¼ˆæœ€å¤š5è½®ï¼‰
            max_iterations = 5
            iteration_count = 0
            
            print(f"è°ƒç”¨ Gemini APIï¼Œæ¨¡å‹: {self.model}ï¼ŒFunction Calling: {len(function_declarations) > 0}")
            
            for iteration in range(max_iterations):
                iteration_count += 1
                
                # è°ƒç”¨ Genai API
                response = self.genai_client.models.generate_content(
                    model=self.model,
                    contents=contents,
                    config=config
                )
                
                # ç»Ÿè®¡ token
                if hasattr(response, 'usage_metadata') and response.usage_metadata:
                    usage = response.usage_metadata
                    if hasattr(usage, 'prompt_token_count'):
                        total_prompt_tokens += usage.prompt_token_count
                    if hasattr(usage, 'candidates_token_count'):
                        total_completion_tokens += usage.candidates_token_count
                    if hasattr(usage, 'total_token_count'):
                        total_tokens += usage.total_token_count
                    print(f"ç¬¬{iteration + 1}è½®è°ƒç”¨ - è¾“å…¥token: {usage.prompt_token_count}, è¾“å‡ºtoken: {usage.candidates_token_count}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å‡½æ•°è°ƒç”¨
                if response.candidates and response.candidates[0].content.parts:
                    has_function_call = False
                    
                    # å°†æ¨¡å‹å“åº”æ·»åŠ åˆ°å¯¹è¯å†å²
                    contents.append(response.candidates[0].content)
                    
                    # å¤„ç†æ¯ä¸ª part
                    for part in response.candidates[0].content.parts:
                        if hasattr(part, 'function_call') and part.function_call:
                            has_function_call = True
                            function_call = part.function_call
                            
                            print(f"ğŸ”§ æ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨: {function_call.name}({dict(function_call.args)})")
                            
                            # æ‰§è¡Œå‡½æ•°
                            function_result = llm_tools.execute_tool_call(
                                function_call.name,
                                dict(function_call.args)
                            )
                            
                            print(f"âœ… å‡½æ•°æ‰§è¡Œç»“æœ: {function_result}")
                            
                            # åˆ›å»ºå‡½æ•°å“åº” part
                            function_response_part = types.Part.from_function_response(
                                name=function_call.name,
                                response={"result": function_result}
                            )
                            
                            # æ·»åŠ å‡½æ•°ç»“æœåˆ°å¯¹è¯å†å²
                            contents.append(types.Content(
                                role="user",
                                parts=[function_response_part]
                            ))
                    
                    # å¦‚æœæ²¡æœ‰å‡½æ•°è°ƒç”¨ï¼Œè¯´æ˜æ¨¡å‹å·²ç»ç”Ÿæˆäº†æœ€ç»ˆå›ç­”
                    if not has_function_call:
                        answer_text = response.text
                        break
                    
                    # ç»§ç»­ä¸‹ä¸€è½®ï¼ˆè®©æ¨¡å‹åŸºäºå‡½æ•°ç»“æœç”Ÿæˆå›ç­”ï¼‰
                else:
                    # æ²¡æœ‰æœ‰æ•ˆå“åº”
                    answer_text = response.text if hasattr(response, 'text') else "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰ç†è§£æ‚¨çš„é—®é¢˜ã€‚"
                    break
            else:
                # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
                print(f"âš ï¸ è­¦å‘Šï¼šè¾¾åˆ°æœ€å¤§å‡½æ•°è°ƒç”¨è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå¼ºåˆ¶è¿”å›")
                answer_text = response.text if hasattr(response, 'text') else "æŠ±æ­‰ï¼Œå¤„ç†æ—¶é—´è¿‡é•¿ã€‚"
            
            # æ‰“å° Token ç»Ÿè®¡
            print(f"=" * 50)
            print(f"æœ¬æ¬¡å¯¹è¯Tokenç»Ÿè®¡:")
            print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
            print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
            print(f"  æ€»è®¡token: {total_tokens}")
            print(f"  å‡½æ•°è°ƒç”¨è½®æ¬¡: {iteration_count}")
            print(f"=" * 50)
            
            return answer_text
            
        except Exception as e:
            print(f"Gemini API è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def chat_with_function_calling(self, user_id, user_message, conversation_history=None):
        """
        ä¸å¤§æ¨¡å‹å¯¹è¯ï¼Œæ”¯æŒFunction Calling
        
        Args:
            user_id: ç”¨æˆ·ID
            user_message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤§æ¨¡å‹çš„å›å¤æ–‡æœ¬
        """
        # å¦‚æœä½¿ç”¨ Google Genai SDKï¼Œè°ƒç”¨ä¸“é—¨çš„æ–¹æ³•
        if self.use_genai_sdk:
            return self._chat_with_genai_sdk(user_id, user_message, conversation_history)
        
        # ä»¥ä¸‹æ˜¯åŸæœ‰çš„ OpenAI SDK å®ç°
        try:
            # Tokenç»Ÿè®¡
            total_prompt_tokens = 0
            total_completion_tokens = 0
            total_tokens = 0
            
            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = []
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            system_prompt = self.prompt_manager.get_prompt('system_prompt')
            if system_prompt:
                messages.append({
                    "role": "system",
                    "content": system_prompt
                })
            
            # æ·»åŠ å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
            if conversation_history:
                messages.extend(conversation_history)
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            messages.append({
                "role": "user",
                "content": user_message
            })
            
            # è°ƒç”¨å¤§æ¨¡å‹
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                tools=TOOLS_SCHEMA,
                tool_choice="auto",
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            # ç»Ÿè®¡ç¬¬ä¸€æ¬¡è°ƒç”¨çš„token
            if hasattr(response, 'usage') and response.usage:
                total_prompt_tokens += response.usage.prompt_tokens
                total_completion_tokens += response.usage.completion_tokens
                total_tokens += response.usage.total_tokens
                print(f"ç¬¬1è½®è°ƒç”¨ - è¾“å…¥token: {response.usage.prompt_tokens}, è¾“å‡ºtoken: {response.usage.completion_tokens}")
            
            # åˆ›å»ºå·¥å…·å®ä¾‹ï¼ˆç”¨äºæ‰§è¡Œå‡½æ•°è°ƒç”¨ï¼‰
            llm_tools = LLMTools(
                self.todo_service, 
                user_id,
                search_client=self.search_client,  # ä¼ é€’ç‹¬ç«‹çš„æœç´¢å®¢æˆ·ç«¯
                search_model=self.search_model,  # ä¼ é€’æœç´¢æ¨¡å‹åç§°
                search_temperature=self.search_temperature  # ä¼ é€’æœç´¢æ¸©åº¦å‚æ•°
            )
            
            # æ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨ï¼ˆæœ€å¤š5è½®ï¼Œé˜²æ­¢æ— é™å¾ªç¯ï¼‰
            max_iterations = 5
            for iteration in range(max_iterations):
                # å¤„ç†å“åº”
                assistant_message = response.choices[0].message
                
                # å¦‚æœå¤§æ¨¡å‹éœ€è¦è°ƒç”¨å‡½æ•°
                if assistant_message.tool_calls:
                    # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯åˆ°å†å²
                    messages.append(assistant_message)
                    
                    # æ‰§è¡Œæ‰€æœ‰å‡½æ•°è°ƒç”¨
                    for tool_call in assistant_message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)
                        
                        print(f"æ‰§è¡Œå‡½æ•°è°ƒç”¨: {function_name}({function_args})")
                        
                        # æ‰§è¡Œå‡½æ•°
                        function_result = llm_tools.execute_tool_call(function_name, function_args)
                        
                        # æ·»åŠ å‡½æ•°ç»“æœåˆ°æ¶ˆæ¯åˆ—è¡¨
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps(function_result, ensure_ascii=False)
                        })
                    
                    # å†æ¬¡è°ƒç”¨å¤§æ¨¡å‹ï¼Œè®©å®ƒåŸºäºå‡½æ•°ç»“æœç”Ÿæˆå›å¤æˆ–ç»§ç»­è°ƒç”¨å·¥å…·
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        tools=TOOLS_SCHEMA,
                        tool_choice="auto",
                        temperature=self.temperature,
                        max_tokens=self.max_tokens
                    )
                    
                    # ç»Ÿè®¡åç»­è°ƒç”¨çš„token
                    if hasattr(response, 'usage') and response.usage:
                        total_prompt_tokens += response.usage.prompt_tokens
                        total_completion_tokens += response.usage.completion_tokens
                        total_tokens += response.usage.total_tokens
                        print(f"ç¬¬{iteration + 2}è½®è°ƒç”¨ - è¾“å…¥token: {response.usage.prompt_tokens}, è¾“å‡ºtoken: {response.usage.completion_tokens}")
                    
                    # ç»§ç»­å¾ªç¯ï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ–°çš„å·¥å…·è°ƒç”¨
                else:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨äº†ï¼Œè¿”å›æœ€ç»ˆå›å¤
                    print(f"=" * 50)
                    print(f"æœ¬æ¬¡å¯¹è¯Tokenç»Ÿè®¡:")
                    print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
                    print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
                    print(f"  æ€»è®¡token: {total_tokens}")
                    print(f"=" * 50)
                    return assistant_message.content
            
            # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œè¿”å›æœ€åçš„å›å¤
            print(f"è­¦å‘Šï¼šè¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨è¿­ä»£æ¬¡æ•°({max_iterations})ï¼Œå¼ºåˆ¶è¿”å›")
            print(f"=" * 50)
            print(f"æœ¬æ¬¡å¯¹è¯Tokenç»Ÿè®¡:")
            print(f"  æ€»è¾“å…¥token: {total_prompt_tokens}")
            print(f"  æ€»è¾“å‡ºtoken: {total_completion_tokens}")
            print(f"  æ€»è®¡token: {total_tokens}")
            print(f"=" * 50)
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}"
    
    def generate_daily_plan(self, user_id):
        """
        ç”Ÿæˆæ¯æ—¥ä»»åŠ¡è§„åˆ’
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            è§„åˆ’æ–‡æœ¬
        """
        try:
            # è·å–æ˜¨å¤©å®Œæˆçš„ä»»åŠ¡
            yesterday_todos = self.todo_service.get_yesterday_completed_todos(user_id)
            yesterday_summary = "\n".join([
                f"- {todo.content}" for todo in yesterday_todos
            ]) if yesterday_todos else "æ— "
            
            # è·å–ä»Šå¤©çš„å¾…åŠä»»åŠ¡
            today_todos = self.todo_service.get_today_todos(user_id)
            today_tasks = "\n".join([
                f"- [{todo.priority}] {todo.content}" for todo in today_todos
            ]) if today_todos else "æ— "
            
            # è·å–è§„åˆ’æç¤ºè¯
            prompt = self.prompt_manager.get_prompt(
                'daily_planning_prompt',
                yesterday_summary=yesterday_summary,
                today_tasks=today_tasks
            )
            
            # è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆè§„åˆ’
            messages = [
                {"role": "system", "content": self.prompt_manager.get_prompt('system_prompt')},
                {"role": "user", "content": prompt}
            ]
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            # ç»Ÿè®¡tokenä½¿ç”¨
            if hasattr(response, 'usage') and response.usage:
                print(f"=" * 50)
                print(f"æ¯æ—¥è§„åˆ’Tokenç»Ÿè®¡:")
                print(f"  è¾“å…¥token: {response.usage.prompt_tokens}")
                print(f"  è¾“å‡ºtoken: {response.usage.completion_tokens}")
                print(f"  æ€»è®¡token: {response.usage.total_tokens}")
                print(f"=" * 50)
            
            return response.choices[0].message.content
            
        except Exception as e:
            print(f"ç”Ÿæˆæ¯æ—¥è§„åˆ’å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆä»Šæ—¥è§„åˆ’ã€‚"

