# -*- coding: utf-8 -*-
"""
ç³»ç»Ÿå‘½ä»¤æœåŠ¡
å¤„ç†ç³»ç»Ÿçº§åˆ«çš„å‘½ä»¤ï¼ˆä¸ç»è¿‡LLMï¼‰
"""


class CommandService:
    """ç³»ç»Ÿå‘½ä»¤æœåŠ¡"""
    
    def __init__(self, conversation_service, todo_service, app_config=None, app_context=None):
        """
        åˆå§‹åŒ–å‘½ä»¤æœåŠ¡
        
        Args:
            conversation_service: å¯¹è¯å†å²æœåŠ¡å®ä¾‹
            todo_service: å¾…åŠäº‹é¡¹æœåŠ¡å®ä¾‹
            app_config: Flask åº”ç”¨é…ç½®ï¼ˆç”¨äºæ¨¡å‹åˆ‡æ¢ï¼‰
            app_context: Flask åº”ç”¨ä¸Šä¸‹æ–‡ï¼ˆç”¨äºè·å–æœåŠ¡å®ä¾‹ï¼‰
        """
        self.conversation_service = conversation_service
        self.todo_service = todo_service
        self.app_config = app_config
        self.app_context = app_context
        
        # å®šä¹‰ç³»ç»Ÿå‘½ä»¤æ˜ å°„ï¼ˆå‘½ä»¤å -> å¤„ç†å‡½æ•°ï¼‰
        self.commands = {
            'cls': self._clear_history,
            'help': self._show_help,
            'å¸®åŠ©': self._show_help,
            'reset': self._reset_all,
            'é‡ç½®': self._reset_all,
            'stats': self._show_stats,
            'ç»Ÿè®¡': self._show_stats,
            'models': self._list_models,
            'model': self._show_current_model,
            'æ¨¡å‹': self._show_current_model,
            'plan': self._generate_plan,
            'è§„åˆ’': self._generate_plan,
        }
    
    def is_system_command(self, message):
        """
        åˆ¤æ–­æ˜¯å¦æ˜¯ç³»ç»Ÿå‘½ä»¤
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            
        Returns:
            æ˜¯å¦æ˜¯ç³»ç»Ÿå‘½ä»¤
        """
        msg = message.strip().lower()
        # æ”¯æŒå›ºå®šå‘½ä»¤
        if msg in self.commands:
            return True
        # æ”¯æŒæ¨¡å‹åˆ‡æ¢å‘½ä»¤ï¼šuse flash, use pro, use promax ç­‰
        if msg.startswith('use '):
            return True
        # æ”¯æŒä¸­æ–‡æ¨¡å‹åˆ‡æ¢å‘½ä»¤
        if msg.startswith('åˆ‡æ¢') or msg.startswith('ä½¿ç”¨'):
            return True
        return False
    
    def execute_command(self, message, user_id):
        """
        æ‰§è¡Œç³»ç»Ÿå‘½ä»¤
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            user_id: ç”¨æˆ·ID
            
        Returns:
            å‘½ä»¤æ‰§è¡Œç»“æœï¼ˆæ–‡æœ¬ï¼‰
        """
        command = message.strip().lower()
        
        # å¤„ç†æ¨¡å‹åˆ‡æ¢å‘½ä»¤
        if command.startswith('use '):
            model_name = command[4:].strip()
            return self._switch_model(user_id, model_name)
        
        if command.startswith('åˆ‡æ¢') or command.startswith('ä½¿ç”¨'):
            # æå–æ¨¡å‹åç§°
            model_name = command.replace('åˆ‡æ¢', '').replace('ä½¿ç”¨', '').strip()
            return self._switch_model(user_id, model_name)
        
        # å¤„ç†å›ºå®šå‘½ä»¤
        if command in self.commands:
            try:
                return self.commands[command](user_id)
            except Exception as e:
                print(f"æ‰§è¡Œå‘½ä»¤ '{command}' å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return f"[sys] âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼š{str(e)}"
        return None
    
    def _clear_history(self, user_id):
        """
        æ¸…ç©ºå¯¹è¯å†å²
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        cleared_count = self.conversation_service.clear_user_history(user_id)
        return f"[sys] âœ… å·²æ¸…ç©ºå¯¹è¯å†å²ï¼\nå…±æ¸…é™¤äº† {cleared_count} æ¡è®°å½•ã€‚"
    
    def _show_help(self, user_id):
        """
        æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            å¸®åŠ©æ–‡æœ¬
        """
        help_text = """[sys] ğŸ“– ç³»ç»Ÿå‘½ä»¤å¸®åŠ©

ğŸ”§ ç³»ç»Ÿå‘½ä»¤ï¼ˆç›´æ¥è¾“å…¥å³å¯ï¼‰ï¼š
â€¢ cls - æ¸…ç©ºå¯¹è¯å†å²
â€¢ help / å¸®åŠ© - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯
â€¢ stats / ç»Ÿè®¡ - æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
â€¢ reset / é‡ç½® - é‡ç½®æ‰€æœ‰æ•°æ®ï¼ˆæ…ç”¨ï¼‰
â€¢ plan / è§„åˆ’ - ç”Ÿæˆä»Šæ˜ä¸¤å¤©çš„ä»»åŠ¡è§„åˆ’

ğŸ¤– æ¨¡å‹ç®¡ç†ï¼š
â€¢ models - æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹
â€¢ model / æ¨¡å‹ - æŸ¥çœ‹å½“å‰æ¨¡å‹
â€¢ use <æ¨¡å‹> - åˆ‡æ¢æ¨¡å‹ï¼ˆå¦‚: use promaxï¼‰
â€¢ åˆ‡æ¢/ä½¿ç”¨ <æ¨¡å‹> - åˆ‡æ¢æ¨¡å‹ï¼ˆä¸­æ–‡ï¼‰

ğŸ’¡ ä½¿ç”¨æç¤ºï¼š
ç›´æ¥å’Œæˆ‘è¯´è¯ï¼Œæˆ‘ä¼šå¸®ä½ ç®¡ç†å¾…åŠäº‹é¡¹ï¼
ä¾‹å¦‚ï¼š
â€¢ "æ˜å¤©ä¸‹åˆå¼€ä¼š"
â€¢ "æŸ¥çœ‹æˆ‘çš„å¾…åŠ"
â€¢ "å®Œæˆç¬¬ä¸€ä¸ªä»»åŠ¡"
"""
        return help_text
    
    def _reset_all(self, user_id):
        """
        é‡ç½®æ‰€æœ‰æ•°æ®ï¼ˆæ…ç”¨ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            æ‰§è¡Œç»“æœ
        """
        # æ¸…ç©ºå¯¹è¯å†å²
        history_count = self.conversation_service.clear_user_history(user_id)
        
        # åˆ é™¤æ‰€æœ‰å¾…åŠäº‹é¡¹
        todos = self.todo_service.get_user_todos(user_id)
        todo_count = len(todos)
        for todo in todos:
            self.todo_service.delete_todo(todo.id, user_id)
        
        return f"""[sys] âš ï¸ å·²é‡ç½®æ‰€æœ‰æ•°æ®ï¼

æ¸…é™¤å†…å®¹ï¼š
â€¢ å¯¹è¯å†å²ï¼š{history_count} æ¡
â€¢ å¾…åŠäº‹é¡¹ï¼š{todo_count} ä¸ª

ç°åœ¨å¯ä»¥é‡æ–°å¼€å§‹ä½¿ç”¨äº†ï¼"""
    
    def _show_stats(self, user_id):
        """
        æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯æ–‡æœ¬
        """
        # è·å–å¯¹è¯å†å²æ•°é‡
        conversation_history = self.conversation_service.get_recent_history(user_id)
        history_count = len(conversation_history)
        
        # è·å–å¾…åŠäº‹é¡¹ç»Ÿè®¡
        all_todos = self.todo_service.get_user_todos(user_id)
        pending_todos = self.todo_service.get_user_todos(user_id, status='pending')
        completed_todos = self.todo_service.get_user_todos(user_id, status='completed')
        
        # è®¡ç®—å®Œæˆç‡
        completion_rate = 0
        if len(all_todos) > 0:
            completion_rate = (len(completed_todos) / len(all_todos)) * 100
        
        stats_text = f"""[sys] ğŸ“Š æ•°æ®ç»Ÿè®¡

ğŸ’¬ å¯¹è¯æ•°æ®ï¼š
â€¢ å†å²å¯¹è¯ï¼š{history_count} æ¡

âœ… å¾…åŠæ•°æ®ï¼š
â€¢ æ€»è®¡ï¼š{len(all_todos)} ä¸ª
â€¢ å¾…å®Œæˆï¼š{len(pending_todos)} ä¸ª
â€¢ å·²å®Œæˆï¼š{len(completed_todos)} ä¸ª
â€¢ å®Œæˆç‡ï¼š{completion_rate:.1f}%

ğŸ¯ ç»§ç»­åŠ æ²¹ï¼"""
        
        return stats_text
    
    def _list_models(self, user_id):
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            æ¨¡å‹åˆ—è¡¨æ–‡æœ¬
        """
        if not self.app_config:
            return "[sys] âŒ æ— æ³•è®¿é—®é…ç½®"
        
        available_models = self.app_config.get('LLM_MODELS', {})
        current_model = self.app_config.get('CURRENT_LLM', 'unknown')
        
        model_list = "[sys] ğŸ¤– å¯ç”¨æ¨¡å‹åˆ—è¡¨\n\n"
        
        for model_key, model_config in available_models.items():
            model_name = model_config.get('model', 'unknown')
            use_search = "âœ…" if model_config.get('use_google_search', False) else "âŒ"
            thinking_budget = model_config.get('thinking_budget')
            
            # æ ‡è®°å½“å‰ä½¿ç”¨çš„æ¨¡å‹
            current_mark = "ğŸ‘‰ " if model_key == current_model else "   "
            
            # æ„å»ºæè¿°
            desc_parts = [f"æ¨¡å‹: {model_name}"]
            desc_parts.append(f"æœç´¢: {use_search}")
            
            if thinking_budget is not None:
                if thinking_budget == -1:
                    desc_parts.append("æ€è€ƒ: åŠ¨æ€")
                elif thinking_budget == 0:
                    desc_parts.append("æ€è€ƒ: å…³é—­")
                else:
                    desc_parts.append(f"æ€è€ƒ: {thinking_budget}")
            
            model_list += f"{current_mark}â€¢ {model_key}\n"
            model_list += f"   {' | '.join(desc_parts)}\n\n"
        
        model_list += "ğŸ’¡ ä½¿ç”¨æ–¹æ³•ï¼š\n"
        model_list += "â€¢ è¾“å…¥ 'use flash' åˆ‡æ¢åˆ° flash æ¨¡å‹\n"
        model_list += "â€¢ è¾“å…¥ 'use promax' åˆ‡æ¢åˆ° promax æ¨¡å‹\n"
        model_list += "â€¢ æˆ–ä½¿ç”¨å®Œæ•´åç§°ï¼Œå¦‚ 'use geminiofficial-pro'"
        
        return model_list
    
    def _show_current_model(self, user_id):
        """
        æ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            å½“å‰æ¨¡å‹ä¿¡æ¯
        """
        if not self.app_config:
            return "[sys] âŒ æ— æ³•è®¿é—®é…ç½®"
        
        current_model_key = self.app_config.get('CURRENT_LLM', 'unknown')
        available_models = self.app_config.get('LLM_MODELS', {})
        
        if current_model_key not in available_models:
            return f"[sys] âš ï¸ å½“å‰æ¨¡å‹: {current_model_key} (é…ç½®ä¸å­˜åœ¨)"
        
        model_config = available_models[current_model_key]
        model_name = model_config.get('model', 'unknown')
        use_search = "å·²å¯ç”¨" if model_config.get('use_google_search', False) else "æœªå¯ç”¨"
        thinking_budget = model_config.get('thinking_budget')
        
        info = f"[sys] ğŸ¤– å½“å‰ä½¿ç”¨çš„æ¨¡å‹\n\n"
        info += f"â€¢ é…ç½®å: {current_model_key}\n"
        info += f"â€¢ æ¨¡å‹: {model_name}\n"
        info += f"â€¢ ç½‘ç»œæœç´¢: {use_search}\n"
        
        if thinking_budget is not None:
            if thinking_budget == -1:
                info += f"â€¢ æ€è€ƒæ¨¡å¼: åŠ¨æ€æ€è€ƒ\n"
            elif thinking_budget == 0:
                info += f"â€¢ æ€è€ƒæ¨¡å¼: å·²å…³é—­\n"
            else:
                info += f"â€¢ æ€è€ƒé¢„ç®—: {thinking_budget} tokens\n"
        
        info += "\nğŸ’¡ è¾“å…¥ 'models' æŸ¥çœ‹æ‰€æœ‰å¯ç”¨æ¨¡å‹"
        
        return info
    
    def _switch_model(self, user_id, model_identifier):
        """
        åˆ‡æ¢æ¨¡å‹
        
        Args:
            user_id: ç”¨æˆ·ID
            model_identifier: æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆå¯ä»¥æ˜¯ç®€ç§°æˆ–å®Œæ•´åç§°ï¼‰
            
        Returns:
            åˆ‡æ¢ç»“æœ
        """
        if not self.app_config or not self.app_context:
            return "[sys] âŒ æ¨¡å‹åˆ‡æ¢åŠŸèƒ½ä¸å¯ç”¨"
        
        available_models = self.app_config.get('LLM_MODELS', {})
        
        # æ¨¡å‹ç®€ç§°æ˜ å°„
        model_shortcuts = {
            'flash': 'geminiofficial-flash',
            'pro': 'geminiofficial-pro',
            'promax': 'geminiofficial-promax',
            'deepseek': 'deepseek',
            'hiapi': 'geminihiapi'
        }
        
        # å°è¯•åŒ¹é…ç®€ç§°æˆ–å®Œæ•´åç§°
        target_model = None
        if model_identifier in model_shortcuts:
            target_model = model_shortcuts[model_identifier]
        elif model_identifier in available_models:
            target_model = model_identifier
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
        if not target_model:
            for key in available_models.keys():
                if model_identifier in key.lower():
                    target_model = key
                    break
        
        if not target_model or target_model not in available_models:
            return f"[sys] âŒ æœªæ‰¾åˆ°æ¨¡å‹: {model_identifier}\n\nğŸ’¡ è¾“å…¥ 'models' æŸ¥çœ‹å¯ç”¨æ¨¡å‹"
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯å½“å‰æ¨¡å‹
        current_model = self.app_config.get('CURRENT_LLM')
        if target_model == current_model:
            return f"[sys] â„¹ï¸ å·²ç»åœ¨ä½¿ç”¨ {target_model} æ¨¡å‹"
        
        try:
            # æ›´æ–°é…ç½®
            self.app_config['CURRENT_LLM'] = target_model
            
            # é‡æ–°åŠ è½½æ¨¡å‹å‚æ•°
            model_config = available_models[target_model]
            self.app_config['LLM_API_KEY'] = model_config['api_key']
            self.app_config['LLM_API_BASE'] = model_config['api_base']
            self.app_config['LLM_MODEL'] = model_config['model']
            self.app_config['LLM_TEMPERATURE'] = model_config['temperature']
            self.app_config['LLM_MAX_TOKENS'] = model_config['max_tokens']
            
            # é‡æ–°åˆå§‹åŒ– LLM æœåŠ¡
            from app.services.llm_service import LLMService
            
            llm_service = LLMService(
                self.app_config,
                self.app_context.prompt_manager,
                self.app_context.todo_service,
                self.app_context.transaction_service  # ä¼ é€’è®°è´¦æœåŠ¡
            )
            
            # æ›´æ–°åº”ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ llm_service
            self.app_context.llm_service = llm_service
            
            # æ„å»ºåˆ‡æ¢æˆåŠŸæ¶ˆæ¯
            model_name = model_config.get('model', target_model)
            use_search = "âœ…" if model_config.get('use_google_search', False) else "âŒ"
            thinking_budget = model_config.get('thinking_budget')
            
            result = f"[sys] âœ… å·²åˆ‡æ¢åˆ°æ¨¡å‹: {target_model}\n\n"
            result += f"â€¢ æ¨¡å‹: {model_name}\n"
            result += f"â€¢ æœç´¢: {use_search}\n"
            
            if thinking_budget is not None:
                if thinking_budget == -1:
                    result += f"â€¢ æ€è€ƒ: åŠ¨æ€\n"
                elif thinking_budget == 0:
                    result += f"â€¢ æ€è€ƒ: å…³é—­\n"
                else:
                    result += f"â€¢ æ€è€ƒ: {thinking_budget} tokens\n"
            
            print(f"âœ… ç”¨æˆ· {user_id} åˆ‡æ¢æ¨¡å‹: {current_model} -> {target_model}")
            
            return result
            
        except Exception as e:
            print(f"âŒ åˆ‡æ¢æ¨¡å‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"[sys] âŒ åˆ‡æ¢æ¨¡å‹å¤±è´¥ï¼š{str(e)}"
    
    def _generate_plan(self, user_id):
        """
        ç”Ÿæˆä»»åŠ¡è§„åˆ’
        
        Args:
            user_id: ç”¨æˆ·ID
            
        Returns:
            è§„åˆ’æ–‡æœ¬
        """
        if not self.app_context:
            return "[sys] âŒ è§„åˆ’åŠŸèƒ½ä¸å¯ç”¨"
        
        try:
            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            from datetime import datetime, timedelta
            import pytz
            
            # è·å–åŒ—äº¬æ—¶é—´
            beijing_tz = pytz.timezone('Asia/Shanghai')
            current_time = datetime.now(beijing_tz)
            
            # è·å–ä»Šå¤©å’Œæ˜å¤©çš„ä»»åŠ¡
            today_todos = self.todo_service.get_today_todos(user_id)
            tomorrow_todos = self.todo_service.get_tomorrow_todos(user_id)
            
            # å¦‚æœæ²¡æœ‰ä»»åŠ¡ï¼Œç»™å‡ºæç¤º
            if not today_todos and not tomorrow_todos:
                return "[sys] ğŸ“ æ‚¨æš‚æ—¶æ²¡æœ‰ä»Šå¤©å’Œæ˜å¤©çš„å¾…åŠä»»åŠ¡ï¼Œå…ˆä¼‘æ¯ä¸€ä¸‹å§ï¼"
            
            # æ„å»ºä»»åŠ¡è¯¦ç»†ä¿¡æ¯
            def format_task_details(todo):
                """æ ¼å¼åŒ–ä»»åŠ¡è¯¦ç»†ä¿¡æ¯"""
                details = []
                details.append(f"â€¢ ä»»åŠ¡ï¼š{todo.content}")
                
                if todo.notes:
                    details.append(f"  å¤‡æ³¨ï¼š{todo.notes}")
                
                if todo.due_date:
                    due_str = todo.due_date.strftime('%Y-%m-%d %H:%M')
                    # è®¡ç®—è·ç¦»ç°åœ¨çš„æ—¶é—´
                    time_diff = todo.due_date - datetime.now()
                    hours = int(time_diff.total_seconds() / 3600)
                    if hours < 0:
                        details.append(f"  æˆªæ­¢æ—¶é—´ï¼š{due_str} (å·²è¶…æ—¶ {abs(hours)} å°æ—¶)")
                    elif hours < 24:
                        details.append(f"  æˆªæ­¢æ—¶é—´ï¼š{due_str} (å‰©ä½™ {hours} å°æ—¶)")
                    else:
                        days = int(hours / 24)
                        details.append(f"  æˆªæ­¢æ—¶é—´ï¼š{due_str} (å‰©ä½™ {days} å¤©)")
                
                created_str = todo.created_at.strftime('%Y-%m-%d %H:%M')
                details.append(f"  åˆ›å»ºæ—¶é—´ï¼š{created_str}")
                
                return "\n".join(details)
            
            # æ ¼å¼åŒ–ä»Šå¤©çš„ä»»åŠ¡
            today_text = ""
            if today_todos:
                today_text = f"ã€ä»Šå¤©çš„å¾…åŠã€‘(å…±{len(today_todos)}ä¸ª)\n"
                today_text += "\n".join([format_task_details(todo) for todo in today_todos])
            else:
                today_text = "ã€ä»Šå¤©çš„å¾…åŠã€‘\næ— "
            
            # æ ¼å¼åŒ–æ˜å¤©çš„ä»»åŠ¡
            tomorrow_text = ""
            if tomorrow_todos:
                tomorrow_text = f"ã€æ˜å¤©çš„å¾…åŠã€‘(å…±{len(tomorrow_todos)}ä¸ª)\n"
                tomorrow_text += "\n".join([format_task_details(todo) for todo in tomorrow_todos])
            else:
                tomorrow_text = "ã€æ˜å¤©çš„å¾…åŠã€‘\næ— "
            
            # è·å– llm_service å¹¶ç”Ÿæˆè§„åˆ’
            llm_service = self.app_context.llm_service
            
            # è·å–è§„åˆ’æç¤ºè¯
            prompt_manager = self.app_context.prompt_manager
            planning_prompt = prompt_manager.get_prompt(
                'task_planning_prompt',
                today_tasks=today_text,
                tomorrow_tasks=tomorrow_text,
                current_time=current_time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')
            )
            
            print(f"=" * 50)
            print(f"ğŸ“‹ ç”Ÿæˆä»»åŠ¡è§„åˆ’ - ç”¨æˆ·ID: {user_id}")
            print(f"ä»Šå¤©ä»»åŠ¡æ•°: {len(today_todos)}")
            print(f"æ˜å¤©ä»»åŠ¡æ•°: {len(tomorrow_todos)}")
            print(f"æç¤ºè¯é•¿åº¦: {len(planning_prompt)} å­—ç¬¦")
            print(f"ä½¿ç”¨æ¨¡å‹: {llm_service.model}")
            print(f"é…ç½®å‚æ•°: temperature={llm_service.temperature}, max_tokens={llm_service.max_tokens}")
            print(f"=" * 50)
            
            # è°ƒç”¨ LLM ç”Ÿæˆè§„åˆ’ï¼ˆä¸ä½¿ç”¨function callingï¼Œçº¯æ–‡æœ¬å¯¹è¯ï¼‰
            # ä½¿ç”¨ OpenAI SDK çš„ç®€å•å¯¹è¯æ¨¡å¼
            if llm_service.use_genai_sdk:
                # ä½¿ç”¨ Google Genai SDK
                from google import genai
                from google.genai import types
                
                # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆæŒ‡å¯¼è¾“å‡ºæ ¼å¼ï¼‰
                system_instruction = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡è§„åˆ’åŠ©æ‰‹ï¼Œå–„äºåˆ†æä»»åŠ¡çš„è½»é‡ç¼“æ€¥ï¼Œåˆ¶å®šç§‘å­¦åˆç†çš„æ‰§è¡Œè®¡åˆ’ã€‚é‡è¦ï¼šè¯·ä½¿ç”¨çº¯æ–‡æœ¬æ ¼å¼è¾“å‡ºï¼Œä¸è¦ä½¿ç”¨Markdownæ ¼å¼ï¼ˆå¦‚**ç²—ä½“**ã€*æ–œä½“*ç­‰ï¼‰ï¼Œä½¿ç”¨emojiå’Œæ¢è¡Œæ¥ç»„ç»‡å†…å®¹ã€‚"
                
                contents = [
                    types.Content(
                        role="user",
                        parts=[types.Part(text=system_instruction + "\n\n" + planning_prompt)]
                    )
                ]
                
                # ä½¿ç”¨å½“å‰æ¨¡å‹çš„é…ç½®ï¼ˆä» llm_service è·å–ï¼‰
                generate_config = types.GenerateContentConfig(
                    temperature=llm_service.temperature,
                    max_output_tokens=llm_service.max_tokens
                )
                
                print(f"ğŸ¤– è°ƒç”¨ Gemini API ç”Ÿæˆä»»åŠ¡è§„åˆ’...")
                response = llm_service.genai_client.models.generate_content(
                    model=llm_service.model,
                    contents=contents,
                    config=generate_config
                )
                
                print(f"ğŸ“¥ æ”¶åˆ° API å“åº”")
                
                # å¥å£®åœ°æå–å“åº”æ–‡æœ¬
                plan_text = ""
                
                # æ–¹å¼1ï¼ˆæ¨èï¼‰: ç›´æ¥ä½¿ç”¨ response.text å±æ€§
                try:
                    if response and hasattr(response, 'text'):
                        plan_text = response.text
                        if plan_text:
                            print(f"   âœ“ ä½¿ç”¨ response.text æˆåŠŸè·å–å†…å®¹")
                except Exception as e:
                    print(f"   âœ— è®¿é—® response.text å¤±è´¥: {e}")
                
                # æ–¹å¼2: å¦‚æœæ–¹å¼1å¤±è´¥ï¼Œä» candidates ä¸­æå–
                if not plan_text:
                    print(f"   â†’ å°è¯•ä» candidates ä¸­æå–...")
                    if response and hasattr(response, 'candidates') and response.candidates:
                        print(f"   âœ“ å“åº”åŒ…å« {len(response.candidates)} ä¸ªå€™é€‰é¡¹")
                        candidate = response.candidates[0]
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰ finish_reasonï¼ˆå¯èƒ½è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢ï¼‰
                        if hasattr(candidate, 'finish_reason'):
                            print(f"   âœ“ å®ŒæˆåŸå› : {candidate.finish_reason}")
                        
                        if hasattr(candidate, 'content') and candidate.content:
                            print(f"   âœ“ å€™é€‰é¡¹åŒ…å«å†…å®¹")
                            # æ£€æŸ¥ parts æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸º None
                            if hasattr(candidate.content, 'parts') and candidate.content.parts:
                                print(f"   âœ“ å†…å®¹åŒ…å« {len(candidate.content.parts)} ä¸ªéƒ¨åˆ†")
                                for part in candidate.content.parts:
                                    if hasattr(part, 'text') and part.text:
                                        plan_text += part.text
                            else:
                                print(f"   âœ— å†…å®¹çš„ parts ä¸ºç©ºæˆ–ä¸å­˜åœ¨")
                        else:
                            print(f"   âœ— å€™é€‰é¡¹å†…å®¹ä¸ºç©º")
                    else:
                        print(f"   âœ— å“åº”æ²¡æœ‰å€™é€‰é¡¹")
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œè¿”å›é»˜è®¤æ¶ˆæ¯
                if not plan_text:
                    plan_text = "æŠ±æ­‰ï¼ŒAI æ— æ³•ç”Ÿæˆä»»åŠ¡è§„åˆ’ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š\n1. API å“åº”å¼‚å¸¸\n2. å†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨æ‹¦æˆª\n\nè¯·ç¨åå†è¯•ï¼Œæˆ–è”ç³»ç®¡ç†å‘˜æ£€æŸ¥æ—¥å¿—ã€‚"
                    print(f"âš ï¸ è­¦å‘Š: æœªèƒ½ä»å“åº”ä¸­æå–æ–‡æœ¬")
                else:
                    print(f"âœ… æˆåŠŸç”Ÿæˆè§„åˆ’æ–‡æœ¬ï¼Œé•¿åº¦: {len(plan_text)} å­—ç¬¦")
                
            else:
                # ä½¿ç”¨ OpenAI SDK
                print(f"ğŸ¤– è°ƒç”¨ OpenAI å…¼å®¹ API ç”Ÿæˆä»»åŠ¡è§„åˆ’...")
                response = llm_service.client.chat.completions.create(
                    model=llm_service.model,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡è§„åˆ’åŠ©æ‰‹ï¼Œå–„äºåˆ†æä»»åŠ¡çš„è½»é‡ç¼“æ€¥ï¼Œåˆ¶å®šç§‘å­¦åˆç†çš„æ‰§è¡Œè®¡åˆ’ã€‚"},
                        {"role": "user", "content": planning_prompt}
                    ],
                    temperature=llm_service.temperature,
                    max_tokens=llm_service.max_tokens
                )
                
                plan_text = response.choices[0].message.content
                if not plan_text:
                    plan_text = "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆä»»åŠ¡è§„åˆ’ã€‚è¯·ç¨åå†è¯•ã€‚"
                    print(f"âš ï¸ è­¦å‘Š: OpenAI å“åº”å†…å®¹ä¸ºç©º")
                else:
                    print(f"âœ… æˆåŠŸç”Ÿæˆè§„åˆ’æ–‡æœ¬ï¼Œé•¿åº¦: {len(plan_text)} å­—ç¬¦")
            
            # æ·»åŠ ç³»ç»Ÿæ ‡è®°
            result = f"[sys] ğŸ“‹ ä»»åŠ¡è§„åˆ’å»ºè®®\n\n{plan_text}"
            
            print(f"âœ… ç”¨æˆ· {user_id} ç”Ÿæˆä»»åŠ¡è§„åˆ’æˆåŠŸ")
            return result
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆä»»åŠ¡è§„åˆ’å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return f"[sys] âŒ ç”Ÿæˆè§„åˆ’å¤±è´¥ï¼š{str(e)}"
    
    def get_all_commands(self):
        """
        è·å–æ‰€æœ‰æ”¯æŒçš„ç³»ç»Ÿå‘½ä»¤
        
        Returns:
            å‘½ä»¤åˆ—è¡¨
        """
        return list(self.commands.keys())

